{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Stage 3: Response Builder - Unit Tests\n",
        "\n",
        "This notebook tests the **Response Builder** service in isolation.\n",
        "\n",
        "**What it tests:**\n",
        "- Agent execution (with mocks)\n",
        "- Financial metrics extraction (mock LLM)\n",
        "- Strategic analysis generation (mock LLM)\n",
        "- Prepared content formatting\n",
        "- Error handling for failed agents"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Setup path\n",
        "import sys\n",
        "sys.path.insert(0, \"../..\")\n",
        "\n",
        "from cmpt.services import (\n",
        "    ResponseBuilderService,\n",
        "    ContextBuilderOutput,\n",
        "    ContentPrioritizationOutput,\n",
        "    CompanyInfo,\n",
        "    TemporalContext,\n",
        "    PrioritizedSource,\n",
        "    Subquery,\n",
        "    DataSource,\n",
        "    Priority,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Helper: Create Mock Inputs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def create_mock_context_output(\n",
        "    company_name: str = \"Apple Inc\",\n",
        "    ticker: str = \"AAPL\",\n",
        ") -> ContextBuilderOutput:\n",
        "    \"\"\"Create mock ContextBuilderOutput.\"\"\"\n",
        "    return ContextBuilderOutput(\n",
        "        company_info=CompanyInfo(\n",
        "            name=company_name,\n",
        "            ticker=ticker,\n",
        "            industry=\"Technology\",\n",
        "            sector=\"Information Technology\",\n",
        "            market_cap=\"Large Cap\",\n",
        "        ),\n",
        "        company_name=company_name,\n",
        "        ticker=ticker,\n",
        "        temporal_context=TemporalContext(\n",
        "            meeting_date=\"2025-02-15\",\n",
        "            fiscal_quarter=\"1\",\n",
        "            fiscal_year=\"2025\",\n",
        "            news_lookback_days=30,\n",
        "            filing_quarters=8,\n",
        "        ),\n",
        "    )\n",
        "\n",
        "\n",
        "def create_mock_prioritization_output(\n",
        "    ticker: str = \"AAPL\",\n",
        ") -> ContentPrioritizationOutput:\n",
        "    \"\"\"Create mock ContentPrioritizationOutput.\"\"\"\n",
        "    return ContentPrioritizationOutput(\n",
        "        prioritized_sources=[\n",
        "            PrioritizedSource(source=DataSource.SEC_FILING, priority=Priority.PRIMARY, enabled=True),\n",
        "            PrioritizedSource(source=DataSource.EARNINGS, priority=Priority.PRIMARY, enabled=True),\n",
        "            PrioritizedSource(source=DataSource.NEWS, priority=Priority.SECONDARY, enabled=True),\n",
        "        ],\n",
        "        subqueries=[\n",
        "            Subquery(agent=\"SEC_agent\", query=\"Apple Inc\", params={\"ticker\": ticker}),\n",
        "            Subquery(agent=\"earnings_agent\", query=\"Apple Inc\", params={\"ticker\": ticker}),\n",
        "            Subquery(agent=\"news_agent\", query=\"Apple Inc\", params={\"ticker\": ticker}),\n",
        "        ],\n",
        "        subqueries_by_agent={\n",
        "            \"SEC_agent\": [Subquery(agent=\"SEC_agent\", query=\"Apple Inc\", params={\"ticker\": ticker})],\n",
        "            \"earnings_agent\": [Subquery(agent=\"earnings_agent\", query=\"Apple Inc\", params={\"ticker\": ticker})],\n",
        "            \"news_agent\": [Subquery(agent=\"news_agent\", query=\"Apple Inc\", params={\"ticker\": ticker})],\n",
        "        },\n",
        "        priority_distribution={\"earnings_agent\": 50, \"news_agent\": 30, \"SEC_agent\": 20},\n",
        "    )\n",
        "\n",
        "print(\"✓ Helper functions created\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Test 1: Basic Execution (No LLM, No Agents)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "async def test_basic_execution():\n",
        "    \"\"\"Test basic service execution without LLM or agents.\"\"\"\n",
        "    # No LLM client, no agents - will use mock/fallback implementations\n",
        "    service = ResponseBuilderService()\n",
        "    \n",
        "    context = create_mock_context_output()\n",
        "    prioritization = create_mock_prioritization_output()\n",
        "    \n",
        "    output = await service.execute(context, prioritization)\n",
        "    \n",
        "    # Verify output structure\n",
        "    assert output is not None\n",
        "    assert output.company_name == \"Apple Inc\"\n",
        "    \n",
        "    print(f\"✓ Company: {output.company_name}\")\n",
        "    print(f\"✓ Agents Succeeded: {output.agents_succeeded}\")\n",
        "    print(f\"✓ Agents Failed: {output.agents_failed}\")\n",
        "    print(f\"✓ Has Financial Metrics: {output.financial_metrics is not None}\")\n",
        "    print(f\"✓ Has Strategic Analysis: {output.strategic_analysis is not None}\")\n",
        "    print(f\"✓ Has Prepared Content: {output.prepared_content is not None}\")\n",
        "    return output\n",
        "\n",
        "output = await test_basic_execution()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Test 2: Prepared Content Structure"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "async def test_prepared_content():\n",
        "    \"\"\"Test that prepared content is properly formatted.\"\"\"\n",
        "    service = ResponseBuilderService()\n",
        "    \n",
        "    context = create_mock_context_output(company_name=\"Microsoft Corporation\", ticker=\"MSFT\")\n",
        "    prioritization = create_mock_prioritization_output(ticker=\"MSFT\")\n",
        "    \n",
        "    output = await service.execute(context, prioritization)\n",
        "    \n",
        "    # Verify prepared content\n",
        "    assert output.prepared_content is not None\n",
        "    \n",
        "    # Should contain company name\n",
        "    assert \"Microsoft\" in output.prepared_content\n",
        "    \n",
        "    print(f\"✓ Prepared Content (first 500 chars):\")\n",
        "    print(\"-\" * 50)\n",
        "    print(output.prepared_content[:500])\n",
        "    print(\"-\" * 50)\n",
        "    return output\n",
        "\n",
        "output = await test_prepared_content()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Test 3: Agent Results Structure"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "async def test_agent_results():\n",
        "    \"\"\"Test that agent results are properly structured.\"\"\"\n",
        "    service = ResponseBuilderService()\n",
        "    \n",
        "    context = create_mock_context_output()\n",
        "    prioritization = create_mock_prioritization_output()\n",
        "    \n",
        "    output = await service.execute(context, prioritization)\n",
        "    \n",
        "    print(f\"✓ Agent Results:\")\n",
        "    for agent_name, result in output.agent_results.items():\n",
        "        print(f\"    {agent_name}:\")\n",
        "        print(f\"      - Success: {result.get('success', 'N/A')}\")\n",
        "        print(f\"      - Has Data: {result.get('data') is not None}\")\n",
        "        print(f\"      - Error: {result.get('error', 'None')}\")\n",
        "    \n",
        "    return output\n",
        "\n",
        "output = await test_agent_results()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Test 4: Financial Metrics Structure"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "async def test_financial_metrics():\n",
        "    \"\"\"Test financial metrics structure (with mock data).\"\"\"\n",
        "    service = ResponseBuilderService()\n",
        "    \n",
        "    context = create_mock_context_output()\n",
        "    prioritization = create_mock_prioritization_output()\n",
        "    \n",
        "    output = await service.execute(context, prioritization)\n",
        "    \n",
        "    if output.financial_metrics:\n",
        "        print(f\"✓ Financial Metrics:\")\n",
        "        for key, value in list(output.financial_metrics.items())[:10]:\n",
        "            if value is not None and not key.endswith('_citation'):\n",
        "                print(f\"    {key}: {value}\")\n",
        "    else:\n",
        "        print(\"✓ No financial metrics (expected without LLM)\")\n",
        "    \n",
        "    return output\n",
        "\n",
        "output = await test_financial_metrics()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Test 5: Strategic Analysis Structure"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "async def test_strategic_analysis():\n",
        "    \"\"\"Test strategic analysis structure (with mock data).\"\"\"\n",
        "    service = ResponseBuilderService()\n",
        "    \n",
        "    context = create_mock_context_output()\n",
        "    prioritization = create_mock_prioritization_output()\n",
        "    \n",
        "    output = await service.execute(context, prioritization)\n",
        "    \n",
        "    if output.strategic_analysis:\n",
        "        print(f\"✓ Strategic Analysis:\")\n",
        "        for key, value in output.strategic_analysis.items():\n",
        "            if isinstance(value, list):\n",
        "                print(f\"    {key}: {len(value)} items\")\n",
        "            elif value:\n",
        "                print(f\"    {key}: {str(value)[:50]}...\")\n",
        "    else:\n",
        "        print(\"✓ No strategic analysis (expected without LLM)\")\n",
        "    \n",
        "    return output\n",
        "\n",
        "output = await test_strategic_analysis()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Test 6: Timing Metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "async def test_timing_metrics():\n",
        "    \"\"\"Test that timing metrics are captured.\"\"\"\n",
        "    service = ResponseBuilderService()\n",
        "    \n",
        "    context = create_mock_context_output()\n",
        "    prioritization = create_mock_prioritization_output()\n",
        "    \n",
        "    output = await service.execute(context, prioritization)\n",
        "    \n",
        "    assert output.timing_ms is not None\n",
        "    \n",
        "    print(f\"✓ Timing Metrics:\")\n",
        "    for key, value in output.timing_ms.items():\n",
        "        print(f\"    {key}: {value:.2f}ms\")\n",
        "    \n",
        "    # Should have total timing\n",
        "    assert \"total\" in output.timing_ms or len(output.timing_ms) > 0\n",
        "    return output\n",
        "\n",
        "output = await test_timing_metrics()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Test 7: Error Handling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "async def test_error_handling():\n",
        "    \"\"\"Test graceful handling of errors.\"\"\"\n",
        "    service = ResponseBuilderService()\n",
        "    \n",
        "    # Minimal context (missing some expected fields)\n",
        "    context = ContextBuilderOutput(\n",
        "        company_name=\"TestCorp\",\n",
        "    )\n",
        "    prioritization = ContentPrioritizationOutput(\n",
        "        prioritized_sources=[],\n",
        "        subqueries=[],\n",
        "        subqueries_by_agent={},\n",
        "    )\n",
        "    \n",
        "    # Should not raise, should handle gracefully\n",
        "    output = await service.execute(context, prioritization)\n",
        "    \n",
        "    assert output is not None\n",
        "    print(f\"✓ Service handled minimal input gracefully\")\n",
        "    print(f\"✓ Company: {output.company_name}\")\n",
        "    print(f\"✓ Errors: {output.errors}\")\n",
        "    return output\n",
        "\n",
        "output = await test_error_handling()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Test 8: Validation Results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "async def test_validation_results():\n",
        "    \"\"\"Test that validation results are included.\"\"\"\n",
        "    service = ResponseBuilderService()\n",
        "    \n",
        "    context = create_mock_context_output()\n",
        "    prioritization = create_mock_prioritization_output()\n",
        "    \n",
        "    output = await service.execute(context, prioritization)\n",
        "    \n",
        "    if output.validation_results:\n",
        "        print(f\"✓ Validation Results:\")\n",
        "        if \"validation_summary\" in output.validation_results:\n",
        "            summary = output.validation_results[\"validation_summary\"]\n",
        "            print(f\"    Fields Checked: {summary.get('total_fields_checked', 0)}\")\n",
        "            print(f\"    Fields with Values: {summary.get('fields_with_values', 0)}\")\n",
        "            print(f\"    Sources Verified: {summary.get('sources_verified', 0)}\")\n",
        "    else:\n",
        "        print(\"✓ No validation results (expected without financial metrics)\")\n",
        "    \n",
        "    return output\n",
        "\n",
        "output = await test_validation_results()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Summary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"=\"*60)\n",
        "print(\"  Stage 3: Response Builder - All Tests Passed!\")\n",
        "print(\"=\"*60)\n",
        "print()\n",
        "print(\"Tested:\")\n",
        "print(\"  ✓ Basic execution (no LLM, no agents)\")\n",
        "print(\"  ✓ Prepared content structure\")\n",
        "print(\"  ✓ Agent results structure\")\n",
        "print(\"  ✓ Financial metrics structure\")\n",
        "print(\"  ✓ Strategic analysis structure\")\n",
        "print(\"  ✓ Timing metrics\")\n",
        "print(\"  ✓ Error handling\")\n",
        "print(\"  ✓ Validation results\")\n",
        "print()\n",
        "print(\"Note: Full LLM/agent tests require live services.\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
