{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# \ud83d\udd25 FlowForge: Client Meeting Prep Tool (CMPT) Chain Implementation\n",
    "\n",
    "## A Comprehensive Tutorial on Building DAG-Based Chain Orchestration\n",
    "\n",
    "---\n",
    "\n",
    "## \ud83d\udccb Table of Contents\n",
    "\n",
    "### Part 1: Understanding FlowForge\n",
    "1. [What is FlowForge?](#1-what-is-flowforge)\n",
    "2. [What FlowForge Provides vs What CMPT Implements](#2-flowforge-vs-cmpt)\n",
    "3. [Architecture Overview](#3-architecture-overview)\n",
    "\n",
    "### Part 2: Setup & Configuration\n",
    "4. [Imports and Dependencies](#4-imports)\n",
    "5. [Data Models & Schemas](#5-data-models)\n",
    "6. [Grid Configuration (Business Rules)](#6-grid-config)\n",
    "\n",
    "### Part 3: FlowForge Core Concepts\n",
    "7. [Initializing FlowForge](#7-init-forge)\n",
    "8. [Registering Data Agents](#8-data-agents)\n",
    "9. [Defining Chain Steps](#9-chain-steps)\n",
    "10. [Creating the Chain](#10-chain-definition)\n",
    "\n",
    "### Part 4: CMPT Pipeline Implementation\n",
    "11. [Context Builder Steps](#11-context-builder)\n",
    "12. [Content Prioritization Steps](#12-content-prioritization)\n",
    "13. [Data Fetching (Parallel Execution)](#13-data-fetch)\n",
    "14. [Response Builder Steps](#14-response-builder)\n",
    "15. [LLM Integration](#15-llm-integration)\n",
    "\n",
    "### Part 5: Execution & Debugging\n",
    "16. [Chain Validation](#16-validation)\n",
    "17. [DAG Visualization](#17-visualization)\n",
    "18. [Running the Chain](#18-execution)\n",
    "19. [Error Handling & Debugging](#19-error-handling)\n",
    "\n",
    "### Part 6: Advanced Features\n",
    "20. [Middleware Integration](#20-middleware)\n",
    "21. [Additional Components from Old Code](#21-resources)\n",
    "    - 21.1 [Metrics Validator](#21-1-metrics-validator)\n",
    "    - 21.2 [Static Subquery Engine](#21-2-static-subquery-engine)\n",
    "    - 21.3 [LLM Prompts](#21-3-llm-prompts)\n",
    "    - 21.4 [Persona Extraction (LDAP & ZoomInfo)](#21-4-persona-extraction)\n",
    "22. [Testing Strategies](#22-testing)\n",
    "23. [Complete Migration Summary & Production Checklist](#23-summary)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"1-what-is-flowforge\"></a>\n",
    "## 1. What is FlowForge?\n",
    "\n",
    "**FlowForge** is a DAG-based Chain Orchestration Framework inspired by Dagster patterns. It provides:\n",
    "\n",
    "| Feature | Description |\n",
    "|---------|-------------|\n",
    "| **Decorator-based API** | Clean `@forge.agent()`, `@forge.step()`, `@forge.chain()` syntax |\n",
    "| **Automatic DAG Resolution** | Dependencies are resolved and parallel execution happens automatically |\n",
    "| **Context Management** | Scoped storage (step/chain/global), token tracking |\n",
    "| **Middleware Pipeline** | Extensible hooks for logging, caching, summarization |\n",
    "| **MCP Integration** | Connect external MCP servers as agents |\n",
    "| **Validation & Visualization** | Built-in chain validation and DAG visualization |\n",
    "\n",
    "### FlowForge Architecture Diagram\n",
    "\n",
    "```\n",
    "\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n",
    "\u2502                                    FlowForge Core                                    \u2502\n",
    "\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n",
    "\u2502                                                                                     \u2502\n",
    "\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510   \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510   \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510                   \u2502\n",
    "\u2502  \u2502   Registries    \u2502   \u2502   DAG Executor  \u2502   \u2502 Context Manager \u2502                   \u2502\n",
    "\u2502  \u2502                 \u2502   \u2502                 \u2502   \u2502                 \u2502                   \u2502\n",
    "\u2502  \u2502 \u2022 AgentRegistry \u2502   \u2502 \u2022 Build DAG     \u2502   \u2502 \u2022 Step Scope    \u2502                   \u2502\n",
    "\u2502  \u2502 \u2022 StepRegistry  \u2502   \u2502 \u2022 Resolve Deps  \u2502   \u2502 \u2022 Chain Scope   \u2502                   \u2502\n",
    "\u2502  \u2502 \u2022 ChainRegistry \u2502   \u2502 \u2022 Parallel Exec \u2502   \u2502 \u2022 Token Track   \u2502                   \u2502\n",
    "\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518   \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518   \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518                   \u2502\n",
    "\u2502           \u2502                    \u2502                      \u2502                            \u2502\n",
    "\u2502           \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518                            \u2502\n",
    "\u2502                                \u2502                                                    \u2502\n",
    "\u2502                    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u25bc\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510                                       \u2502\n",
    "\u2502                    \u2502   Middleware Layer    \u2502                                       \u2502\n",
    "\u2502                    \u2502  (Logging, Caching)   \u2502                                       \u2502\n",
    "\u2502                    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518                                       \u2502\n",
    "\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "<a id=\"2-flowforge-vs-cmpt\"></a>\n## 2. What FlowForge Provides vs What CMPT Implements\n\n### \ud83d\udd27 FlowForge Provides (Framework Layer)\n\n| Component | What FlowForge Handles |\n|-----------|------------------------|\n| **DAG Execution Engine** | Automatic dependency resolution, topological sorting, parallel execution |\n| **Step Registration** | `@forge.step()` decorator with deps, produces, timeout, retry |\n| **Agent Registration** | `@forge.agent()` decorator for data source classes |\n| **Chain Definition** | `@forge.chain()` decorator for defining execution pipelines |\n| **Context Management** | `ChainContext` with scoped storage (STEP, CHAIN, GLOBAL) |\n| **Resource Management** | Lifecycle management for DB connections, HTTP clients |\n| **Validation** | `forge.check()` validates chains, dependencies, cycles |\n| **Visualization** | `forge.graph()` generates ASCII/Mermaid DAG diagrams |\n| **Error Handling** | fail_fast vs continue modes, retry logic |\n| **Middleware** | Pre/post hooks for logging, caching, metrics |\n\n---\n\n### \ud83d\udcd6 DAG Execution Engine Explained\n\n**DAG** = **D**irected **A**cyclic **G**raph - a structure where tasks flow in one direction with no cycles.\n\nThe DAG Execution Engine does three things **automatically**:\n\n| Capability | Description |\n|------------|-------------|\n| **1. Dependency Resolution** | Figures out which steps need which other steps to complete first |\n| **2. Topological Sorting** | Orders steps so dependencies always run before dependents |\n| **3. Parallel Execution** | Runs independent steps simultaneously to save time |\n\n#### CMPT Example: Data Fetching\n\nWhen you define steps like this:\n\n```python\n@forge.step(name=\"fetch_news_data\", deps=[\"prioritize_content\"])\n@forge.step(name=\"fetch_sec_data\", deps=[\"prioritize_content\"])  \n@forge.step(name=\"fetch_earnings_data\", deps=[\"prioritize_content\"])\n@forge.step(name=\"parse_agent_data\", deps=[\"fetch_news_data\", \"fetch_sec_data\", \"fetch_earnings_data\"])\n```\n\n**FlowForge automatically figures out:**\n\n| Step | What FlowForge Determines |\n|------|---------------------------|\n| **Dependency Resolution** | `fetch_news_data`, `fetch_sec_data`, `fetch_earnings_data` all need `prioritize_content` first |\n| **Topological Sort** | `prioritize_content` \u2192 `[fetch_news, fetch_sec, fetch_earnings]` \u2192 `parse_agent_data` |\n| **Parallel Execution** | All three fetch steps share the same dependency, so they run **simultaneously** |\n\n#### Visual Comparison: Sequential vs Parallel\n\n**\u274c Without FlowForge (Old Code - Sequential):**\n```\nprioritize_content    (1 sec)\n       \u2193\nfetch_news_data       (2 sec)\n       \u2193\nfetch_sec_data        (2 sec)\n       \u2193\nfetch_earnings_data   (2 sec)\n       \u2193\nparse_agent_data      (1 sec)\n\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\nTotal: 8 seconds\n```\n\n**\u2705 With FlowForge (Automatic Parallelization):**\n```\nprioritize_content         (1 sec)\n       \u2193\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2193      \u2193      \u2193\nfetch_ fetch_ fetch_        (2 sec - all three run simultaneously!)\nnews   sec    earnings\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n       \u2193\nparse_agent_data           (1 sec)\n\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\nTotal: 4 seconds (50% faster!)\n```\n\n#### Another CMPT Example: LLM Calls\n\n```python\n@forge.step(name=\"generate_financial_metrics\", deps=[\"build_prompts\"])\n@forge.step(name=\"generate_strategic_analysis\", deps=[\"build_prompts\"])\n```\n\nBoth LLM calls depend on `build_prompts`, so FlowForge runs them **in parallel**:\n\n```\nbuild_prompts                    (0.5 sec)\n       \u2193\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2193             \u2193\ngenerate_     generate_          (10 sec - both run simultaneously!)\nfinancial_    strategic_\nmetrics       analysis\n\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\nTotal: ~10 sec instead of 20+ sec\n```\n\n#### \ud83d\udca1 Key Insight\n\n> **You don't write parallelization code.** You just declare dependencies with `deps=[]`, and FlowForge:\n> 1. Builds the DAG automatically\n> 2. Identifies which steps can run in parallel  \n> 3. Manages the execution with asyncio\n\n---\n\n### \ud83d\udcca CMPT Implements (Business Logic Layer)\n\n| Component | What CMPT Chain Implements |\n|-----------|----------------------------|\n| **Data Agents** | NewsAgent, SECAgent, EarningsAgent (MCP integrations) |\n| **Context Builder** | Company info extraction, temporal context, persona extraction |\n| **Content Prioritization** | Temporal source prioritizer, subquery engine, topic ranker |\n| **Response Builder** | Prompt construction, LLM calls, metrics validation |\n| **Business Rules** | Grid config, priority profiles, earnings proximity rules |\n| **Data Models** | FinancialMetricsResponse, StrategicAnalysisResponse, CitationDict |\n\n### Visual Comparison: Framework vs Business Logic\n\n```\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                        CMPT CHAIN APPLICATION                         \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502  Business Logic Layer (CMPT Implements)                               \u2502\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510              \u2502\n\u2502  \u2502 Data Agents \u2502  \u2502 Grid Config  \u2502  \u2502 LLM Prompts     \u2502              \u2502\n\u2502  \u2502 \u2022 News      \u2502  \u2502 \u2022 Priorities \u2502  \u2502 \u2022 Metrics       \u2502              \u2502\n\u2502  \u2502 \u2022 SEC       \u2502  \u2502 \u2022 Rules      \u2502  \u2502 \u2022 Analysis      \u2502              \u2502\n\u2502  \u2502 \u2022 Earnings  \u2502  \u2502 \u2022 Topics     \u2502  \u2502 \u2022 Validation    \u2502              \u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518              \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502  Framework Layer (FlowForge Provides)                                 \u2502\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510      \u2502\n\u2502  \u2502 Decorators \u2502  \u2502 DAG Engine \u2502  \u2502 Context    \u2502  \u2502 Middleware \u2502      \u2502\n\u2502  \u2502 @step      \u2502  \u2502 Parallel   \u2502  \u2502 Management \u2502  \u2502 Logging    \u2502      \u2502\n\u2502  \u2502 @agent     \u2502  \u2502 Execution  \u2502  \u2502 Scopes     \u2502  \u2502 Caching    \u2502      \u2502\n\u2502  \u2502 @chain     \u2502  \u2502 Retry      \u2502  \u2502 Tokens     \u2502  \u2502 Metrics    \u2502      \u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518      \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n```"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"3-architecture-overview\"></a>\n",
    "## 3. CMPT Chain Architecture Overview\n",
    "\n",
    "The CMPT pipeline follows this execution flow:\n",
    "\n",
    "```\n",
    "\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n",
    "\u2502                     CLIENT MEETING PREP CHAIN SERVER                                \u2502\n",
    "\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n",
    "\u2502                                                                                     \u2502\n",
    "\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u2502\n",
    "\u2502  \u2502   CONTEXT BUILDER    \u2502    \u2502CONTENT PRIORITIZATION   \u2502    \u2502 RESPONSE BUILDER & \u2502  \u2502\n",
    "\u2502  \u2502                      \u2502    \u2502       ENGINE            \u2502    \u2502    GENERATOR       \u2502  \u2502\n",
    "\u2502  \u2502                      \u2502    \u2502                         \u2502    \u2502                    \u2502  \u2502\n",
    "\u2502  \u2502 \u2022 Company Firm       \u2502    \u2502 \u2022 Temporal Source       \u2502    \u2502 \u2022 Agent Execution  \u2502  \u2502\n",
    "\u2502  \u2502   Extractor          \u2502    \u2502   Prioritizer           \u2502    \u2502                    \u2502  \u2502\n",
    "\u2502  \u2502                      \u2502(2) \u2502                         \u2502(3) \u2502 \u2022 Prompt Builder   \u2502  \u2502\n",
    "\u2502  \u2502 \u2022 RBC Persona        \u2502\u2500\u2500\u2500\u25b6\u2502 \u2022 Subquery Engine       \u2502\u2500\u2500\u2500\u25b6\u2502                    \u2502  \u2502\n",
    "\u2502  \u2502   Extractor          \u2502    \u2502                         \u2502    \u2502 \u2022 Response Builder \u2502  \u2502\n",
    "\u2502  \u2502                      \u2502    \u2502 \u2022 Topic Ranker          \u2502    \u2502                    \u2502  \u2502\n",
    "\u2502  \u2502 \u2022 Temporal Context   \u2502    \u2502                         \u2502    \u2502 \u2022 Validation       \u2502  \u2502\n",
    "\u2502  \u2502   Extractor          \u2502    \u2502 \u2022 Grid Config           \u2502    \u2502                    \u2502  \u2502\n",
    "\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2502\n",
    "\u2502                                                                        \u2502 (5)        \u2502\n",
    "\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n",
    "                                                                         \u2502\n",
    "                         \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n",
    "                         \u2502 (4)\n",
    "                         \u25bc\n",
    "\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n",
    "\u2502                                  DATA AGENTS                                        \u2502\n",
    "\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n",
    "\u2502    SEC Filing      \u2502       News          \u2502              Earnings                    \u2502\n",
    "\u2502                    \u2502                     \u2502                                          \u2502\n",
    "\u2502   >> RavenPack     \u2502    >> RavenPack     \u2502              FACTSET                     \u2502\n",
    "\u2502                    \u2502                     \u2502                                          \u2502\n",
    "\u2502 Last 8 quarters    \u2502 1 year (market cap) \u2502              Latest                      \u2502\n",
    "\u2502 (revenue),         \u2502 30 days (others)    \u2502                                          \u2502\n",
    "\u2502 Latest (Others)    \u2502                     \u2502                                          \u2502\n",
    "\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n",
    "```\n",
    "\n",
    "### Data Flow Sequence\n",
    "\n",
    "1. **User Request \u2192 Context Builder**: User provides meeting details (client, date, attendees)\n",
    "2. **Context Builder \u2192 Content Prioritization**: Extracted context flows to prioritization engine\n",
    "3. **Content Prioritization \u2192 Response Builder**: Prioritized content with subqueries flows to generator\n",
    "4. **Response Builder \u2192 Data Agents**: Agent execution queries external data sources (PARALLEL)\n",
    "5. **Data Agents \u2192 Response Builder**: Retrieved data flows back for response building\n",
    "6. **Response Builder \u2192 LLM**: Generate financial metrics and strategic analysis (PARALLEL)\n",
    "7. **LLM \u2192 Response**: Final CMPT content returned to user"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"4-imports\"></a>\n",
    "## 4. Imports and Dependencies\n",
    "\n",
    "Let's start by importing all necessary modules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\n",
    "#                           IMPORTS & DEPENDENCIES\n",
    "# \u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\n",
    "\n",
    "from __future__ import annotations\n",
    "\n",
    "# Standard library\n",
    "import asyncio\n",
    "import logging\n",
    "import os\n",
    "import re\n",
    "import json\n",
    "from dataclasses import dataclass, field\n",
    "from datetime import datetime, timedelta\n",
    "from enum import Enum\n",
    "from typing import Any, Optional, Dict, List\n",
    "from difflib import SequenceMatcher\n",
    "\n",
    "# Third-party\n",
    "from pydantic import BaseModel, Field\n",
    "import httpx  # For HTTP requests\n",
    "\n",
    "# FlowForge - The framework we're using\n",
    "from flowforge import FlowForge\n",
    "from flowforge.core.context import ChainContext, ContextScope\n",
    "\n",
    "# Setup logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "print(\"\u2705 All imports successful!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"5-data-models\"></a>\n",
    "## 5. Data Models & Schemas\n",
    "\n",
    "These Pydantic models define the structure of our data. They ensure type safety and provide validation.\n",
    "\n",
    "### Key Models:\n",
    "- **CMPTRequest**: Input request for the chain\n",
    "- **CitationDict**: Source tracking for LLM extractions\n",
    "- **FinancialMetricsResponse**: Financial data with citations\n",
    "- **StrategicAnalysisResponse**: SWOT, investment thesis, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\n",
    "#                           DATA MODELS & SCHEMAS\n",
    "# \u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\n",
    "\n",
    "class ToolName(Enum):\n",
    "    \"\"\"\n",
    "    Data agent tool names - maps to MCP tool endpoints.\n",
    "    \n",
    "    These are the three primary data sources for CMPT:\n",
    "    - EARNINGS: Quarterly earnings transcripts from FACTSET\n",
    "    - NEWS: Recent news articles from RavenPack\n",
    "    - SEC: SEC filings (10-K, 10-Q) from RavenPack\n",
    "    \"\"\"\n",
    "    EARNINGS_TOOL = \"earnings_agent\"\n",
    "    NEWS_TOOL = \"news_agent\"\n",
    "    SEC_TOOL = \"SEC_agent\"  # Note: Capital letters to match old code\n",
    "\n",
    "\n",
    "class CMPTRequest(BaseModel):\n",
    "    \"\"\"\n",
    "    Unified request model for CMPT chain.\n",
    "    \n",
    "    This matches the original ChainServerRequest from the old codebase.\n",
    "    All fields are optional to support flexible input.\n",
    "    \"\"\"\n",
    "    corporate_client_email: Optional[str] = None\n",
    "    corporate_client_names: Optional[str] = None\n",
    "    rbc_employee_email: Optional[str] = None\n",
    "    meeting_datetime: Optional[str] = None\n",
    "    corporate_company_name: Optional[str] = None\n",
    "    verbose: bool = False\n",
    "\n",
    "\n",
    "class CitationDict(BaseModel):\n",
    "    \"\"\"\n",
    "    Structured citation with source tracking.\n",
    "    \n",
    "    This is CRITICAL for validation - every extracted metric must\n",
    "    have a citation that can be verified against source documents.\n",
    "    \n",
    "    Example:\n",
    "        {\n",
    "            \"source_agent\": [\"SEC_agent\"],\n",
    "            \"source_content\": [\"Total revenues were $96,773 million...\"],\n",
    "            \"reasoning\": \"Extracted from 10-Q filing dated 2025-10-20\"\n",
    "        }\n",
    "    \"\"\"\n",
    "    source_agent: List[str] = Field(\n",
    "        description=\"List of data agents used (SEC_agent, earnings_agent, news_agent)\"\n",
    "    )\n",
    "    source_content: List[str] = Field(\n",
    "        description=\"List of VERBATIM quotes from source chunks - DO NOT PARAPHRASE\"\n",
    "    )\n",
    "    reasoning: str = Field(\n",
    "        description=\"Explanation of extraction/calculation logic with actual numbers\"\n",
    "    )\n",
    "\n",
    "\n",
    "class FinancialMetricsResponse(BaseModel):\n",
    "    \"\"\"\n",
    "    Financial metrics extraction response with full citations.\n",
    "    \n",
    "    Each metric has:\n",
    "    - Value field (float or None)\n",
    "    - Date field (when applicable)\n",
    "    - Citation field (ALWAYS required, even for null values)\n",
    "    \n",
    "    Key metrics:\n",
    "    - Revenue (current, YoY change, next year estimate)\n",
    "    - EBITDA margin (with YoY change)\n",
    "    - Stock price (with daily and YoY changes)\n",
    "    - Market cap\n",
    "    - Revenue growth trajectory (last 7 quarters)\n",
    "    \"\"\"\n",
    "    # Revenue metrics\n",
    "    current_annual_revenue: Optional[float] = None\n",
    "    current_annual_revenue_date: Optional[str] = None\n",
    "    current_annual_revenue_citation: Optional[CitationDict] = None\n",
    "    current_annual_revenue_yoy_change: Optional[float] = None\n",
    "    current_annual_revenue_yoy_change_citation: Optional[CitationDict] = None\n",
    "    estimated_annual_revenue_next_year: Optional[float] = None\n",
    "    estimated_annual_revenue_next_year_date: Optional[str] = None\n",
    "    estimated_annual_revenue_next_year_citation: Optional[CitationDict] = None\n",
    "    \n",
    "    # EBITDA metrics\n",
    "    ebitda_margin: Optional[float] = None\n",
    "    ebitda_margin_citation: Optional[CitationDict] = None\n",
    "    ebitda_margin_yoy_change: Optional[float] = None\n",
    "    ebitda_margin_yoy_change_citation: Optional[CitationDict] = None\n",
    "    \n",
    "    # Stock metrics\n",
    "    stock_price: Optional[float] = None\n",
    "    stock_price_citation: Optional[CitationDict] = None\n",
    "    stock_price_daily_change: Optional[float] = None\n",
    "    stock_price_daily_change_percent: Optional[float] = None\n",
    "    stock_price_yoy_change: Optional[float] = None\n",
    "    stock_price_yoy_change_citation: Optional[CitationDict] = None\n",
    "    \n",
    "    # Market cap\n",
    "    market_cap: Optional[float] = None\n",
    "    market_cap_citation: Optional[CitationDict] = None\n",
    "    market_cap_date: Optional[str] = None\n",
    "    \n",
    "    # Quarterly trajectory\n",
    "    revenue_growth_trajectory: Optional[Dict[str, Optional[float]]] = None\n",
    "    revenue_growth_trajectory_citation: Optional[CitationDict] = None\n",
    "\n",
    "\n",
    "class StrategicAnalysisResponse(BaseModel):\n",
    "    \"\"\"\n",
    "    Strategic analysis response for client meeting prep.\n",
    "    \n",
    "    Includes:\n",
    "    - SWOT analysis (strengths, weaknesses, opportunities, threats)\n",
    "    - Investment thesis\n",
    "    - Key risks\n",
    "    - Strategic opportunities\n",
    "    - Recent developments (with dates and source URLs)\n",
    "    \"\"\"\n",
    "    strength: List[str] = Field(description=\"4-6 key competitive strengths\")\n",
    "    weakness: List[str] = Field(description=\"4-6 key vulnerabilities\")\n",
    "    opportunity: List[str] = Field(description=\"4-6 growth opportunities\")\n",
    "    threat: List[str] = Field(description=\"4-6 external threats\")\n",
    "    \n",
    "    investment_thesis: List[Dict[str, List[str]]] = Field(\n",
    "        description=\"3-4 investment thesis points with subheadings and bullets\"\n",
    "    )\n",
    "    \n",
    "    key_risk_highlights: List[str] = Field(description=\"5-7 critical risks\")\n",
    "    \n",
    "    strategic_opportunities: List[Dict[str, List[str]]] = Field(\n",
    "        description=\"3-4 strategic opportunities for M&A, capital raising, etc.\"\n",
    "    )\n",
    "    \n",
    "    recent_developments: List[Dict[str, Any]] = Field(\n",
    "        description=\"4-6 recent developments with category, date, description, source_url\"\n",
    "    )\n",
    "    \n",
    "    sources: List[str] = Field(description=\"8-12 sources cited\")\n",
    "\n",
    "\n",
    "print(\"\u2705 Data models defined!\")\n",
    "print(f\"   - ToolName enum with {len(ToolName)} agents\")\n",
    "print(f\"   - CMPTRequest for chain input\")\n",
    "print(f\"   - CitationDict for source tracking\")\n",
    "print(f\"   - FinancialMetricsResponse with {len(FinancialMetricsResponse.model_fields)} fields\")\n",
    "print(f\"   - StrategicAnalysisResponse with {len(StrategicAnalysisResponse.model_fields)} fields\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"6-grid-config\"></a>\n",
    "## 6. Grid Configuration (Business Rules)\n",
    "\n",
    "The Grid Configuration defines the business rules for content prioritization.\n",
    "\n",
    "### Key Concepts:\n",
    "- **Priority Profiles**: How much weight each data source gets (earnings vs news vs SEC)\n",
    "- **Temporal Rules**: Rules that change priorities based on meeting date and earnings proximity\n",
    "- **Topics**: Content prioritization topics for ranking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\n",
    "#                           GRID CONFIGURATION\n",
    "# \u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\n",
    "\n",
    "\"\"\"\n",
    "GRID CONFIG: Business rules for content prioritization.\n",
    "\n",
    "This is where the \"intelligence\" of the CMPT chain lives.\n",
    "The rules determine which data sources get priority based on:\n",
    "- Company type (public vs private)\n",
    "- Proximity to earnings date\n",
    "- Meeting date\n",
    "\"\"\"\n",
    "\n",
    "GRID_CONFIG = {\n",
    "    # How many weeks before/after earnings to use \"earnings_dominant\" profile\n",
    "    \"earnings_proximity_weeks\": 1,\n",
    "    \n",
    "    # Priority profiles - percentage weights for each data source\n",
    "    \"priority_profiles\": {\n",
    "        \"earnings_dominant\": {\n",
    "            # When meeting is near earnings, prioritize earnings transcripts\n",
    "            ToolName.EARNINGS_TOOL.value: 50,\n",
    "            ToolName.NEWS_TOOL.value: 30,\n",
    "            ToolName.SEC_TOOL.value: 20\n",
    "        },\n",
    "        \"news_dominant\": {\n",
    "            # Default profile - news is most relevant for recent context\n",
    "            ToolName.EARNINGS_TOOL.value: 20,\n",
    "            ToolName.NEWS_TOOL.value: 60,\n",
    "            ToolName.SEC_TOOL.value: 20\n",
    "        }\n",
    "    },\n",
    "    \n",
    "    # Temporal source prioritizer rules (evaluated in order, first match wins)\n",
    "    \"temporal_source_prioritizer\": {\n",
    "        \"rules\": [\n",
    "            {\n",
    "                # Rule 1: Non-public companies don't have SEC/earnings data\n",
    "                \"name\": \"non_public_company\",\n",
    "                \"condition\": lambda ctx: ctx[\"company_type\"] not in [\"PUB\", \"SUB\"],\n",
    "                \"priority_profile\": \"news_dominant\"\n",
    "            },\n",
    "            {\n",
    "                # Rule 2: Public companies near earnings date\n",
    "                \"name\": \"earnings_proximity\",\n",
    "                \"condition\": lambda ctx: (\n",
    "                    ctx[\"company_type\"] in [\"PUB\", \"SUB\"] and\n",
    "                    ctx.get(\"max_earnings_event_date\") and\n",
    "                    abs((ctx[\"max_earnings_event_date\"] - ctx[\"meeting_date\"]).days) <= ctx[\"window\"]\n",
    "                ),\n",
    "                \"priority_profile\": \"earnings_dominant\"\n",
    "            }\n",
    "        ],\n",
    "        \"default_profile\": \"news_dominant\"\n",
    "    },\n",
    "    \n",
    "    # Topics for content prioritization/ranking\n",
    "    \"content_prioritization_topics\": [\n",
    "        \"financial_performance\",\n",
    "        \"strategic_initiatives\",\n",
    "        \"market_position\",\n",
    "        \"risk_factors\",\n",
    "        \"management_changes\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "print(\"\u2705 Grid configuration loaded!\")\n",
    "print(f\"   - Earnings proximity window: {GRID_CONFIG['earnings_proximity_weeks']} weeks\")\n",
    "print(f\"   - Priority profiles: {list(GRID_CONFIG['priority_profiles'].keys())}\")\n",
    "print(f\"   - Temporal rules: {[r['name'] for r in GRID_CONFIG['temporal_source_prioritizer']['rules']]}\")\n",
    "print(f\"   - Content topics: {GRID_CONFIG['content_prioritization_topics']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"7-init-forge\"></a>\n",
    "## 7. Initializing FlowForge\n",
    "\n",
    "Now we create our FlowForge instance. This is the central orchestrator.\n",
    "\n",
    "### Key Parameters:\n",
    "- `name`: Identifier for this forge instance\n",
    "- `version`: Version string for tracking\n",
    "- `max_parallel`: Maximum concurrent step executions (uses semaphore)\n",
    "- `default_timeout_ms`: Default timeout for steps\n",
    "- `isolated`: Whether to use isolated registries (prevents state bleed between tests)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\n",
    "#                           INITIALIZE FLOWFORGE\n",
    "# \u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\n",
    "\n",
    "\"\"\"\n",
    "Create the FlowForge instance.\n",
    "\n",
    "FlowForge is the central orchestrator that:\n",
    "1. Registers agents, steps, and chains via decorators\n",
    "2. Resolves dependencies and builds the DAG\n",
    "3. Executes steps with parallel optimization\n",
    "4. Manages context across step executions\n",
    "5. Provides validation and visualization tools\n",
    "\"\"\"\n",
    "\n",
    "forge = FlowForge(\n",
    "    name=\"cmpt_chain\",           # Name of this forge instance\n",
    "    version=\"2.0.0\",             # Version for tracking\n",
    "    max_parallel=10,             # Max concurrent step executions\n",
    "    default_timeout_ms=60000,    # 60 second default timeout\n",
    "    isolated=True,               # Use isolated registries (good for testing)\n",
    ")\n",
    "\n",
    "print(f\"\u2705 FlowForge initialized!\")\n",
    "print(f\"   - Name: {forge.name}\")\n",
    "print(f\"   - Version: {forge.version}\")\n",
    "print(f\"   - Isolated mode: {forge._isolated}\")\n",
    "print(f\"\\n\ud83d\udccc FlowForge is now ready to register agents, steps, and chains!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"8-data-agents\"></a>\n",
    "## 8. Registering Data Agents\n",
    "\n",
    "Data Agents are classes that fetch data from external sources (MCP servers, APIs, etc.).\n",
    "\n",
    "### FlowForge Agent Features:\n",
    "- Registered with `@forge.agent()` decorator\n",
    "- Can have name, group, description\n",
    "- Retrieved via `forge.get_agent(\"name\")`\n",
    "- Typically async methods for data fetching\n",
    "\n",
    "### CMPT Agents:\n",
    "1. **NewsAgent**: Fetches news from RavenPack via MCP\n",
    "2. **SECAgent**: Fetches SEC filings from RavenPack via MCP\n",
    "3. **EarningsAgent**: Fetches earnings transcripts from FACTSET via MCP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\n",
    "#                           DATA AGENTS\n",
    "# \u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\n",
    "\n",
    "@forge.agent(name=\"news_agent\", group=\"data\", description=\"Fetches news articles via MCP\")\n",
    "class NewsAgent:\n",
    "    \"\"\"\n",
    "    News data agent using MCP integration.\n",
    "    \n",
    "    Connects to RavenPack MCP server to fetch:\n",
    "    - Recent news articles (last 30 days)\n",
    "    - Market cap/stock price mentions (last 5 days)\n",
    "    - Executive/leadership changes (last 30 days)\n",
    "    - M&A activity\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        # Configuration from environment variables\n",
    "        self.server_url = os.getenv(\"NEWS_AGENT_MCP_URL\")\n",
    "        self.bearer_token = os.getenv(\"NEWS_AGENT_MCP_BEARER_TOKEN\")\n",
    "        self.tool_name = os.getenv(\"NEWS_AGENT_MCP_TOOL\", \"search_news\")\n",
    "\n",
    "    async def fetch(self, company_name: str, **kwargs) -> List[Dict]:\n",
    "        \"\"\"\n",
    "        Fetch news for a company.\n",
    "        \n",
    "        Args:\n",
    "            company_name: Name of the company to search\n",
    "            \n",
    "        Returns:\n",
    "            List of news article chunks with metadata\n",
    "        \"\"\"\n",
    "        subqueries = self._build_subqueries(company_name)\n",
    "        results = []\n",
    "\n",
    "        for subquery in subqueries:\n",
    "            try:\n",
    "                result = await self._execute_query(subquery)\n",
    "                if result:\n",
    "                    results.extend(result)\n",
    "            except Exception as e:\n",
    "                logger.error(f\"News query failed: {e}\")\n",
    "\n",
    "        return results\n",
    "\n",
    "    def _build_subqueries(self, company_name: str) -> List[Dict]:\n",
    "        \"\"\"\n",
    "        Build news search subqueries.\n",
    "        \n",
    "        Creates targeted queries for different types of news:\n",
    "        - Executive changes\n",
    "        - M&A activity\n",
    "        - Stock/market cap updates\n",
    "        \"\"\"\n",
    "        end_date = datetime.now().strftime('%Y-%m-%d')\n",
    "        month_ago = (datetime.now() - timedelta(days=31)).strftime('%Y-%m-%d')\n",
    "        five_days_ago = (datetime.now() - timedelta(days=5)).strftime('%Y-%m-%d')\n",
    "\n",
    "        return [\n",
    "            {\n",
    "                \"search_query\": f\"{company_name} executive leadership changes\",\n",
    "                \"topics\": [\"executive\", \"CEO\", \"CFO\", \"leadership\"],\n",
    "                \"absolute_date_range\": {\"start_date\": month_ago, \"end_date\": end_date}\n",
    "            },\n",
    "            {\n",
    "                \"search_query\": f\"{company_name} mergers acquisitions M&A\",\n",
    "                \"topics\": [\"merger\", \"acquisition\", \"M&A\", \"deal\"],\n",
    "                \"absolute_date_range\": {\"start_date\": month_ago, \"end_date\": end_date}\n",
    "            },\n",
    "            {\n",
    "                \"search_query\": f\"{company_name} stock price market cap\",\n",
    "                \"topics\": [\"market cap\", \"stock price\", \"valuation\"],\n",
    "                \"absolute_date_range\": {\"start_date\": five_days_ago, \"end_date\": end_date}\n",
    "            },\n",
    "        ]\n",
    "\n",
    "    async def _execute_query(self, subquery: Dict) -> List[Dict]:\n",
    "        \"\"\"Execute MCP query - placeholder for real implementation\"\"\"\n",
    "        # In production, this would use the MCP client:\n",
    "        # async with streamablehttp_client(self.server_url, headers=headers) as (read_stream, write_stream, _):\n",
    "        #     async with ClientSession(read_stream, write_stream) as session:\n",
    "        #         await session.initialize()\n",
    "        #         result = await session.call_tool(name=self.tool_name, arguments=subquery)\n",
    "        logger.info(f\"Executing news query: {subquery.get('search_query', '')[:50]}...\")\n",
    "        return []\n",
    "\n",
    "\n",
    "@forge.agent(name=\"sec_agent\", group=\"data\", description=\"Fetches SEC filings via MCP\")\n",
    "class SECAgent:\n",
    "    \"\"\"\n",
    "    SEC filings data agent.\n",
    "    \n",
    "    Connects to RavenPack MCP server to fetch:\n",
    "    - 10-K annual reports\n",
    "    - 10-Q quarterly reports\n",
    "    - Last 8 quarters of revenue data\n",
    "    - Balance sheet data\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.server_url = os.getenv(\"SEC_AGENT_MCP_URL\")\n",
    "        self.bearer_token = os.getenv(\"SEC_AGENT_MCP_BEARER_TOKEN\")\n",
    "        self.tool_name = os.getenv(\"SEC_AGENT_MCP_TOOL\", \"search_filings\")\n",
    "\n",
    "    async def fetch(self, company_name: str, **kwargs) -> List[Dict]:\n",
    "        \"\"\"Fetch SEC filings for a company\"\"\"\n",
    "        subqueries = self._build_subqueries(company_name)\n",
    "        results = []\n",
    "\n",
    "        for subquery in subqueries:\n",
    "            try:\n",
    "                result = await self._execute_query(subquery)\n",
    "                if result:\n",
    "                    results.extend(result)\n",
    "            except Exception as e:\n",
    "                logger.error(f\"SEC query failed: {e}\")\n",
    "\n",
    "        return results\n",
    "\n",
    "    def _build_subqueries(self, company_name: str) -> List[Dict]:\n",
    "        \"\"\"\n",
    "        Build SEC search subqueries.\n",
    "        \n",
    "        Creates queries for:\n",
    "        - Income statements (revenue, last 8 quarters)\n",
    "        - Balance sheets (shares outstanding, latest)\n",
    "        \"\"\"\n",
    "        return [\n",
    "            {\n",
    "                \"reporting_entity\": company_name,\n",
    "                \"search_queries\": [\n",
    "                    \"consolidated statements of operations\",\n",
    "                    \"statements of income\",\n",
    "                    \"total revenue\"\n",
    "                ],\n",
    "                \"keywords\": [\n",
    "                    \"net sales\", \"total revenue\", \"quarterly revenue\",\n",
    "                    \"three months ended\", \"revenue\"\n",
    "                ],\n",
    "                \"get_latest\": 8  # Last 8 quarters for trajectory\n",
    "            },\n",
    "            {\n",
    "                \"reporting_entity\": company_name,\n",
    "                \"search_queries\": [\n",
    "                    \"consolidated balance sheets\",\n",
    "                    \"stockholders equity\",\n",
    "                    \"common stock outstanding shares\"\n",
    "                ],\n",
    "                \"keywords\": [\n",
    "                    \"outstanding shares\", \"common stock\", \"shares outstanding\"\n",
    "                ],\n",
    "                \"get_latest\": 1  # Just latest for market cap calc\n",
    "            }\n",
    "        ]\n",
    "\n",
    "    async def _execute_query(self, subquery: Dict) -> List[Dict]:\n",
    "        \"\"\"Execute MCP query - placeholder\"\"\"\n",
    "        logger.info(f\"Executing SEC query for: {subquery.get('reporting_entity', '')}\")\n",
    "        return []\n",
    "\n",
    "\n",
    "@forge.agent(name=\"earnings_agent\", group=\"data\", description=\"Fetches earnings transcripts via MCP\")\n",
    "class EarningsAgent:\n",
    "    \"\"\"\n",
    "    Earnings transcript data agent.\n",
    "    \n",
    "    Connects to FACTSET MCP server to fetch:\n",
    "    - Quarterly earnings call transcripts\n",
    "    - Management commentary\n",
    "    - Forward guidance\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.server_url = os.getenv(\"EARNINGS_AGENT_MCP_URL\")\n",
    "        self.bearer_token = os.getenv(\"EARNINGS_AGENT_MCP_BEARER_TOKEN\")\n",
    "        self.tool_name = os.getenv(\"EARNINGS_AGENT_MCP_TOOL\", \"search_earnings\")\n",
    "\n",
    "    async def fetch(\n",
    "        self,\n",
    "        company_name: str,\n",
    "        fiscal_year: str,\n",
    "        fiscal_quarter: str,\n",
    "        **kwargs\n",
    "    ) -> List[Dict]:\n",
    "        \"\"\"\n",
    "        Fetch earnings transcripts for a company.\n",
    "        \n",
    "        Args:\n",
    "            company_name: Company name\n",
    "            fiscal_year: e.g., \"2025\"\n",
    "            fiscal_quarter: e.g., \"1\", \"2\", \"3\", \"4\"\n",
    "        \"\"\"\n",
    "        try:\n",
    "            subquery = {\n",
    "                \"query\": f\"Give me the earnings transcript for {company_name} for fiscal year: {fiscal_year} and quarter: {fiscal_quarter}.\"\n",
    "            }\n",
    "            return await self._execute_query(subquery)\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Earnings query failed: {e}\")\n",
    "            return []\n",
    "\n",
    "    async def _execute_query(self, subquery: Dict) -> List[Dict]:\n",
    "        \"\"\"Execute MCP query - placeholder\"\"\"\n",
    "        logger.info(f\"Executing earnings query: {subquery.get('query', '')[:50]}...\")\n",
    "        return []\n",
    "\n",
    "\n",
    "print(\"\u2705 Data agents registered!\")\n",
    "print(f\"   - Agents: {forge.list_agents()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"9-chain-steps\"></a>\n",
    "## 9. Defining Chain Steps (Overview)\n",
    "\n",
    "Steps are the building blocks of a chain. Each step:\n",
    "- Takes a `ChainContext` as input\n",
    "- Can read/write to the context\n",
    "- Declares its dependencies and what it produces\n",
    "- Returns a result dictionary\n",
    "\n",
    "### FlowForge Step Decorator Parameters:\n",
    "\n",
    "| Parameter | Description | Example |\n",
    "|-----------|-------------|----------|\n",
    "| `name` | Unique step identifier | `\"extract_company_info\"` |\n",
    "| `deps` | List of dependency step names | `[\"build_context\"]` |\n",
    "| `produces` | Context keys this step produces | `[\"company_info\"]` |\n",
    "| `description` | Human-readable description | `\"Extract company info\"` |\n",
    "| `group` | Step group for organization | `\"context_builder\"` |\n",
    "| `timeout_ms` | Execution timeout | `30000` |\n",
    "| `retry` | Number of retries on failure | `3` |\n",
    "| `max_concurrency` | Limit parallel instances | `2` |\n",
    "| `resources` | Resources to inject | `[\"db\", \"llm\"]` |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"11-context-builder\"></a>\n",
    "## 11. Context Builder Steps\n",
    "\n",
    "The Context Builder extracts information needed for the pipeline:\n",
    "1. **Company Info**: Ticker, company type, sector from foundation service\n",
    "2. **Temporal Context**: Earnings calendar, fiscal year/quarter\n",
    "3. **Persona Info** (optional): RBC employee and client personas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\n",
    "#                       CONTEXT BUILDER STEPS\n",
    "# \u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\n",
    "\n",
    "@forge.step(\n",
    "    name=\"extract_company_info\",\n",
    "    produces=[\"company_info\"],\n",
    "    description=\"Extract company information from external API\",\n",
    "    timeout_ms=30000,\n",
    "    group=\"context_builder\"\n",
    ")\n",
    "async def extract_company_info(ctx: ChainContext) -> Dict:\n",
    "    \"\"\"\n",
    "    Extract company information (ticker, type, etc.) from foundation service.\n",
    "    \n",
    "    This step:\n",
    "    1. Reads company_name from context (provided in initial data)\n",
    "    2. Calls the foundation company matches API\n",
    "    3. Stores the result in context for downstream steps\n",
    "    \n",
    "    Equivalent to: ContextBuilderService.corporate_client_firm_extractor\n",
    "    \"\"\"\n",
    "    company_name = ctx.get(\"company_name\")\n",
    "\n",
    "    if not company_name:\n",
    "        logger.warning(\"No company name provided\")\n",
    "        return {\"company_info\": None}\n",
    "\n",
    "    try:\n",
    "        # In production, this would call the foundation service:\n",
    "        # url = os.getenv('FOUNDATION_COMPANY_MATCHES')\n",
    "        # async with httpx.AsyncClient(verify=False, timeout=20.0) as client:\n",
    "        #     response = await client.post(url, json={\"company_name\": company_name})\n",
    "        #     result = response.json()\n",
    "        #     if result.get(\"result\", {}).get(\"matches\"):\n",
    "        #         company_info = result[\"result\"][\"matches\"][0]\n",
    "\n",
    "        # Placeholder response structure matching old code\n",
    "        company_info = {\n",
    "            \"company_name\": company_name,\n",
    "            \"ticker_symbol\": None,\n",
    "            \"company_type\": \"PUB\",  # PUB, PRIV, SUB\n",
    "            \"sector\": None,\n",
    "            \"industry\": None,\n",
    "        }\n",
    "\n",
    "        # Store in context for downstream steps\n",
    "        ctx.set(\"company_info\", company_info, scope=ContextScope.CHAIN)\n",
    "        logger.info(f\"Extracted company info for: {company_name}\")\n",
    "\n",
    "        return {\"company_info\": company_info}\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Company info extraction failed: {e}\")\n",
    "        return {\"company_info\": None, \"error\": str(e)}\n",
    "\n",
    "\n",
    "@forge.step(\n",
    "    name=\"extract_temporal_context\",\n",
    "    deps=[\"extract_company_info\"],  # Depends on company info\n",
    "    produces=[\"temporal_context\"],\n",
    "    description=\"Extract temporal context (earnings calendar) for the company\",\n",
    "    timeout_ms=30000,\n",
    "    group=\"context_builder\"\n",
    ")\n",
    "async def extract_temporal_context(ctx: ChainContext) -> Dict:\n",
    "    \"\"\"\n",
    "    Extract temporal context including earnings calendar.\n",
    "    \n",
    "    Uses company info from previous step to:\n",
    "    1. Look up upcoming earnings dates\n",
    "    2. Determine fiscal year/quarter\n",
    "    3. Find the closest earnings event to meeting date\n",
    "    \n",
    "    Equivalent to: ContextBuilderService.temporal_content_extractor\n",
    "    \"\"\"\n",
    "    company_info = ctx.get(\"company_info\", {})\n",
    "    company_name = company_info.get(\"company_name\") or ctx.get(\"company_name\")\n",
    "    ticker = company_info.get(\"ticker_symbol\")\n",
    "\n",
    "    try:\n",
    "        # In production:\n",
    "        # url = os.getenv('FOUNDATION_EARNING_CALENDAR_URL')\n",
    "        # payload = {\"top_n\": GRID_CONFIG[\"earnings_proximity_weeks\"]}\n",
    "        # if company_name: payload[\"company_name\"] = company_name\n",
    "        # elif ticker: payload[\"ticker_symbol\"] = ticker\n",
    "        # response = await client.post(url, json=payload)\n",
    "        # result = response.json()\n",
    "        # # Parse to find latest event_dt\n",
    "\n",
    "        # Placeholder response\n",
    "        temporal_context = {\n",
    "            \"event_dt\": None,  # Next earnings date\n",
    "            \"fiscal_year\": str(datetime.now().year),\n",
    "            \"fiscal_period\": str((datetime.now().month - 1) // 3 + 1),\n",
    "            \"earnings_events\": []\n",
    "        }\n",
    "\n",
    "        ctx.set(\"temporal_context\", temporal_context, scope=ContextScope.CHAIN)\n",
    "        logger.info(f\"Extracted temporal context for: {company_name}\")\n",
    "\n",
    "        return {\"temporal_context\": temporal_context}\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Temporal context extraction failed: {e}\")\n",
    "        return {\"temporal_context\": None, \"error\": str(e)}\n",
    "\n",
    "\n",
    "@forge.step(\n",
    "    name=\"build_context\",\n",
    "    deps=[\"extract_company_info\", \"extract_temporal_context\"],  # Fan-in point\n",
    "    produces=[\"context_builder_output\"],\n",
    "    description=\"Combine all context builder outputs\",\n",
    "    group=\"context_builder\"\n",
    ")\n",
    "async def build_context(ctx: ChainContext) -> Dict:\n",
    "    \"\"\"\n",
    "    Combine all context builder outputs into a unified structure.\n",
    "    \n",
    "    This is the \"fan-in\" step that waits for all context extraction\n",
    "    to complete and creates the final context builder output.\n",
    "    \n",
    "    Equivalent to: ContextBuilderService.execute\n",
    "    \"\"\"\n",
    "    company_info = ctx.get(\"company_info\", {})\n",
    "    temporal_context = ctx.get(\"temporal_context\", {})\n",
    "    meeting_date = ctx.get(\"meeting_date\") or datetime.now().strftime(\"%Y-%m-%d\")\n",
    "\n",
    "    context_builder_output = {\n",
    "        \"company_info\": company_info,\n",
    "        \"temporal_context\": temporal_context,\n",
    "        \"meeting_date\": meeting_date,\n",
    "        \"company_name\": company_info.get(\"company_name\") or ctx.get(\"company_name\"),\n",
    "        \"company_type\": company_info.get(\"company_type\", \"PUB\"),\n",
    "        # These would be populated if persona extraction was enabled\n",
    "        \"rbc_persona\": None,\n",
    "        \"corporate_client_persona\": None,\n",
    "    }\n",
    "\n",
    "    ctx.set(\"context_builder_output\", context_builder_output, scope=ContextScope.CHAIN)\n",
    "\n",
    "    return {\"context_builder_output\": context_builder_output}\n",
    "\n",
    "\n",
    "print(\"\u2705 Context builder steps registered!\")\n",
    "print(f\"   - Steps: extract_company_info -> extract_temporal_context -> build_context\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"12-content-prioritization\"></a>\n",
    "## 12. Content Prioritization Steps\n",
    "\n",
    "The Content Prioritization engine determines:\n",
    "1. **Source Priorities**: Which data sources get more weight\n",
    "2. **Subqueries**: What queries to send to each agent\n",
    "3. **Topic Rankings**: Which topics are most important"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\n",
    "#                    CONTENT PRIORITIZATION STEPS\n",
    "# \u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\n",
    "\n",
    "@forge.step(\n",
    "    name=\"temporal_source_prioritizer\",\n",
    "    deps=[\"build_context\"],\n",
    "    produces=[\"source_priorities\"],\n",
    "    description=\"Determine data source priorities based on temporal context\",\n",
    "    group=\"content_prioritization\"\n",
    ")\n",
    "async def temporal_source_prioritizer(ctx: ChainContext) -> Dict:\n",
    "    \"\"\"\n",
    "    Determine data source priorities based on meeting date and earnings proximity.\n",
    "    \n",
    "    This implements the business rules from GRID_CONFIG:\n",
    "    - If meeting is near earnings date -> earnings_dominant profile\n",
    "    - If company is private -> news_dominant profile\n",
    "    - Otherwise -> default profile\n",
    "    \n",
    "    Equivalent to: ContentPrioritizationService.temporal_source_prioritizer\n",
    "    \"\"\"\n",
    "    context_output = ctx.get(\"context_builder_output\", {})\n",
    "    temporal_context = context_output.get(\"temporal_context\", {})\n",
    "    meeting_date = context_output.get(\"meeting_date\")\n",
    "    company_type = context_output.get(\"company_type\", \"PUB\")\n",
    "\n",
    "    config = GRID_CONFIG\n",
    "    earnings_proximity_weeks = config.get(\"earnings_proximity_weeks\", 1)\n",
    "    window = earnings_proximity_weeks * 7  # Convert to days\n",
    "\n",
    "    # Parse dates\n",
    "    meeting_datetime = datetime.strptime(meeting_date, \"%Y-%m-%d\") if meeting_date else datetime.now()\n",
    "    max_earnings_date = None\n",
    "    if temporal_context.get(\"event_dt\"):\n",
    "        max_earnings_date = datetime.strptime(temporal_context[\"event_dt\"], \"%Y-%m-%d\")\n",
    "\n",
    "    # Build rule context for evaluation\n",
    "    rule_context = {\n",
    "        \"company_type\": company_type,\n",
    "        \"meeting_date\": meeting_datetime,\n",
    "        \"max_earnings_event_date\": max_earnings_date,\n",
    "        \"window\": window\n",
    "    }\n",
    "\n",
    "    # Evaluate rules from grid config\n",
    "    prioritizer_config = config[\"temporal_source_prioritizer\"]\n",
    "    priority_profiles = config[\"priority_profiles\"]\n",
    "    default_profile = prioritizer_config[\"default_profile\"]\n",
    "    source_priorities = priority_profiles[default_profile]\n",
    "\n",
    "    # Check each rule in order - first match wins\n",
    "    for rule in prioritizer_config[\"rules\"]:\n",
    "        try:\n",
    "            if callable(rule[\"condition\"]) and rule[\"condition\"](rule_context):\n",
    "                profile_name = rule[\"priority_profile\"]\n",
    "                source_priorities = priority_profiles[profile_name]\n",
    "                logger.info(f\"Applied priority rule: {rule['name']} -> {profile_name}\")\n",
    "                break\n",
    "        except Exception as e:\n",
    "            logger.warning(f\"Error evaluating rule '{rule.get('name')}': {e}\")\n",
    "\n",
    "    ctx.set(\"source_priorities\", source_priorities, scope=ContextScope.CHAIN)\n",
    "\n",
    "    return {\"source_priorities\": source_priorities}\n",
    "\n",
    "\n",
    "@forge.step(\n",
    "    name=\"generate_subqueries\",\n",
    "    deps=[\"build_context\"],  # Same dep as prioritizer - can run in parallel!\n",
    "    produces=[\"subqueries\"],\n",
    "    description=\"Generate subqueries for data agents\",\n",
    "    group=\"content_prioritization\"\n",
    ")\n",
    "async def generate_subqueries(ctx: ChainContext) -> Dict:\n",
    "    \"\"\"\n",
    "    Generate subqueries for each data agent.\n",
    "    \n",
    "    Uses the agent classes to build their specific subqueries\n",
    "    based on the company context.\n",
    "    \n",
    "    Equivalent to: ContentPrioritizationService.subquery_engine\n",
    "    \"\"\"\n",
    "    context_output = ctx.get(\"context_builder_output\", {})\n",
    "    company_name = context_output.get(\"company_name\", \"\")\n",
    "    temporal_context = context_output.get(\"temporal_context\", {})\n",
    "\n",
    "    fiscal_year = temporal_context.get(\"fiscal_year\", str(datetime.now().year))\n",
    "    fiscal_quarter = temporal_context.get(\"fiscal_period\", \"1\")\n",
    "\n",
    "    # Get agents and build their subqueries\n",
    "    news_agent = forge.get_agent(\"news_agent\")\n",
    "    sec_agent = forge.get_agent(\"sec_agent\")\n",
    "\n",
    "    subqueries = {\n",
    "        ToolName.NEWS_TOOL.value: news_agent._build_subqueries(company_name),\n",
    "        ToolName.SEC_TOOL.value: sec_agent._build_subqueries(company_name),\n",
    "        ToolName.EARNINGS_TOOL.value: [{\n",
    "            \"company_name\": company_name,\n",
    "            \"fiscal_year\": fiscal_year,\n",
    "            \"fiscal_quarter\": fiscal_quarter\n",
    "        }]\n",
    "    }\n",
    "\n",
    "    ctx.set(\"subqueries\", subqueries, scope=ContextScope.CHAIN)\n",
    "\n",
    "    return {\"subqueries\": subqueries}\n",
    "\n",
    "\n",
    "@forge.step(\n",
    "    name=\"prioritize_content\",\n",
    "    deps=[\"temporal_source_prioritizer\", \"generate_subqueries\"],  # Fan-in\n",
    "    produces=[\"content_prioritization_output\"],\n",
    "    description=\"Combine prioritization outputs\",\n",
    "    group=\"content_prioritization\"\n",
    ")\n",
    "async def prioritize_content(ctx: ChainContext) -> Dict:\n",
    "    \"\"\"\n",
    "    Combine all content prioritization outputs.\n",
    "    \n",
    "    Equivalent to: ContentPrioritizationService.execute\n",
    "    \"\"\"\n",
    "    source_priorities = ctx.get(\"source_priorities\", {})\n",
    "    subqueries = ctx.get(\"subqueries\", {})\n",
    "\n",
    "    content_prioritization_output = {\n",
    "        \"temporal_source_prioritizer\": source_priorities,\n",
    "        \"subqueries_from_engine\": subqueries,\n",
    "        \"topic_ranker_result\": GRID_CONFIG.get(\"content_prioritization_topics\", [])\n",
    "    }\n",
    "\n",
    "    ctx.set(\"content_prioritization_output\", content_prioritization_output, scope=ContextScope.CHAIN)\n",
    "\n",
    "    return {\"content_prioritization_output\": content_prioritization_output}\n",
    "\n",
    "\n",
    "print(\"\u2705 Content prioritization steps registered!\")\n",
    "print(f\"   - temporal_source_prioritizer: Applies business rules\")\n",
    "print(f\"   - generate_subqueries: Creates agent queries\")\n",
    "print(f\"   - prioritize_content: Combines outputs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"13-data-fetch\"></a>\n",
    "## 13. Data Fetching Steps (Parallel Execution)\n",
    "\n",
    "These steps fetch data from the external agents. Since they all depend on the same `prioritize_content` step, **FlowForge will execute them in parallel automatically!**\n",
    "\n",
    "```\n",
    "                    prioritize_content\n",
    "                           \u2502\n",
    "           \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n",
    "           \u2502               \u2502               \u2502\n",
    "           \u25bc               \u25bc               \u25bc\n",
    "    fetch_news_data  fetch_sec_data  fetch_earnings_data\n",
    "           \u2502               \u2502               \u2502\n",
    "           \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n",
    "                           \u2502\n",
    "                           \u25bc\n",
    "                    parse_agent_data\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\n",
    "#                    DATA FETCHING STEPS (PARALLEL)\n",
    "# \u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\n",
    "\n",
    "@forge.step(\n",
    "    name=\"fetch_news_data\",\n",
    "    deps=[\"prioritize_content\"],\n",
    "    produces=[\"news_data\"],\n",
    "    description=\"Fetch news data from news agent\",\n",
    "    timeout_ms=60000,\n",
    "    group=\"data_fetch\"\n",
    ")\n",
    "async def fetch_news_data(ctx: ChainContext) -> Dict:\n",
    "    \"\"\"\n",
    "    Fetch news data using the news agent.\n",
    "    \n",
    "    This step runs in PARALLEL with fetch_sec_data and fetch_earnings_data\n",
    "    because they all share the same dependency.\n",
    "    \"\"\"\n",
    "    context_output = ctx.get(\"context_builder_output\", {})\n",
    "    company_name = context_output.get(\"company_name\", \"\")\n",
    "\n",
    "    try:\n",
    "        news_agent = forge.get_agent(\"news_agent\")\n",
    "        news_data = await news_agent.fetch(company_name)\n",
    "        ctx.set(\"news_data\", news_data, scope=ContextScope.CHAIN)\n",
    "        logger.info(f\"Fetched {len(news_data)} news items\")\n",
    "        return {\"news_data\": news_data}\n",
    "    except Exception as e:\n",
    "        logger.error(f\"News data fetch failed: {e}\")\n",
    "        return {\"news_data\": [], \"error\": str(e)}\n",
    "\n",
    "\n",
    "@forge.step(\n",
    "    name=\"fetch_sec_data\",\n",
    "    deps=[\"prioritize_content\"],  # Same dep = parallel execution!\n",
    "    produces=[\"sec_data\"],\n",
    "    description=\"Fetch SEC filings data\",\n",
    "    timeout_ms=60000,\n",
    "    group=\"data_fetch\"\n",
    ")\n",
    "async def fetch_sec_data(ctx: ChainContext) -> Dict:\n",
    "    \"\"\"Fetch SEC filings data using the SEC agent\"\"\"\n",
    "    context_output = ctx.get(\"context_builder_output\", {})\n",
    "    company_name = context_output.get(\"company_name\", \"\")\n",
    "\n",
    "    try:\n",
    "        sec_agent = forge.get_agent(\"sec_agent\")\n",
    "        sec_data = await sec_agent.fetch(company_name)\n",
    "        ctx.set(\"sec_data\", sec_data, scope=ContextScope.CHAIN)\n",
    "        logger.info(f\"Fetched {len(sec_data)} SEC filings\")\n",
    "        return {\"sec_data\": sec_data}\n",
    "    except Exception as e:\n",
    "        logger.error(f\"SEC data fetch failed: {e}\")\n",
    "        return {\"sec_data\": [], \"error\": str(e)}\n",
    "\n",
    "\n",
    "@forge.step(\n",
    "    name=\"fetch_earnings_data\",\n",
    "    deps=[\"prioritize_content\"],  # Same dep = parallel execution!\n",
    "    produces=[\"earnings_data\"],\n",
    "    description=\"Fetch earnings transcript data\",\n",
    "    timeout_ms=60000,\n",
    "    group=\"data_fetch\"\n",
    ")\n",
    "async def fetch_earnings_data(ctx: ChainContext) -> Dict:\n",
    "    \"\"\"Fetch earnings data using the earnings agent\"\"\"\n",
    "    context_output = ctx.get(\"context_builder_output\", {})\n",
    "    temporal_context = context_output.get(\"temporal_context\", {})\n",
    "    company_name = context_output.get(\"company_name\", \"\")\n",
    "\n",
    "    fiscal_year = temporal_context.get(\"fiscal_year\", str(datetime.now().year))\n",
    "    fiscal_quarter = temporal_context.get(\"fiscal_period\", \"1\")\n",
    "\n",
    "    try:\n",
    "        earnings_agent = forge.get_agent(\"earnings_agent\")\n",
    "        earnings_data = await earnings_agent.fetch(\n",
    "            company_name,\n",
    "            fiscal_year=fiscal_year,\n",
    "            fiscal_quarter=fiscal_quarter\n",
    "        )\n",
    "        ctx.set(\"earnings_data\", earnings_data, scope=ContextScope.CHAIN)\n",
    "        logger.info(f\"Fetched {len(earnings_data)} earnings items\")\n",
    "        return {\"earnings_data\": earnings_data}\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Earnings data fetch failed: {e}\")\n",
    "        return {\"earnings_data\": [], \"error\": str(e)}\n",
    "\n",
    "\n",
    "print(\"\u2705 Data fetching steps registered!\")\n",
    "print(f\"   - fetch_news_data, fetch_sec_data, fetch_earnings_data\")\n",
    "print(f\"   - These will run in PARALLEL since they share the same dependency!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"14-response-builder\"></a>\n",
    "## 14. Response Builder Steps\n",
    "\n",
    "The Response Builder takes the fetched data and:\n",
    "1. Parses and formats the agent data\n",
    "2. Builds LLM prompts\n",
    "3. Calls LLM for metrics and analysis (parallel)\n",
    "4. Validates the results\n",
    "5. Builds the final response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\n",
    "#                       RESPONSE BUILDER STEPS\n",
    "# \u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\n",
    "\n",
    "@forge.step(\n",
    "    name=\"parse_agent_data\",\n",
    "    deps=[\"fetch_news_data\", \"fetch_sec_data\", \"fetch_earnings_data\"],  # Fan-in from parallel\n",
    "    produces=[\"parsed_agent_data\"],\n",
    "    description=\"Parse and format data from all agents\",\n",
    "    group=\"response_builder\"\n",
    ")\n",
    "async def parse_agent_data(ctx: ChainContext) -> Dict:\n",
    "    \"\"\"\n",
    "    Parse and format data from all agents into a unified structure.\n",
    "    \n",
    "    This step formats the raw MCP responses into a readable format\n",
    "    for the LLM prompts.\n",
    "    \n",
    "    Equivalent to: ResponseBuilderAndGenerator.context_parser\n",
    "    \"\"\"\n",
    "    news_data = ctx.get(\"news_data\", [])\n",
    "    sec_data = ctx.get(\"sec_data\", [])\n",
    "    earnings_data = ctx.get(\"earnings_data\", [])\n",
    "\n",
    "    def format_chunks(data: List, agent_name: str) -> str:\n",
    "        \"\"\"\n",
    "        Format data chunks into readable text for LLM.\n",
    "        \n",
    "        Output format:\n",
    "        CHUNK-1\n",
    "        \n",
    "        METADATA\n",
    "        key1: value1\n",
    "        key2: value2\n",
    "        \n",
    "        CHUNK-CONTENT\n",
    "        The actual text content...\n",
    "        \"\"\"\n",
    "        if not data:\n",
    "            return \"\"\n",
    "\n",
    "        formatted_chunks = []\n",
    "        for idx, item in enumerate(data, 1):\n",
    "            if isinstance(item, dict):\n",
    "                chunk_str = f\"CHUNK-{idx}\\n\\n\"\n",
    "                chunk_str += \"METADATA\\n\"\n",
    "\n",
    "                # Extract metadata (everything except 'text')\n",
    "                metadata = {k: v for k, v in item.items() if k != \"text\"}\n",
    "                for k, v in metadata.items():\n",
    "                    chunk_str += f\"{k}: {v}\\n\"\n",
    "\n",
    "                chunk_str += \"\\n\\nCHUNK-CONTENT\\n\"\n",
    "                chunk_str += item.get(\"text\", str(item))\n",
    "                formatted_chunks.append(chunk_str)\n",
    "\n",
    "        return \"\\n\\n\\n\".join(formatted_chunks)\n",
    "\n",
    "    parsed_agent_data = {\n",
    "        ToolName.NEWS_TOOL.value: format_chunks(news_data, \"news\"),\n",
    "        ToolName.SEC_TOOL.value: format_chunks(sec_data, \"sec\"),\n",
    "        ToolName.EARNINGS_TOOL.value: format_chunks(earnings_data, \"earnings\"),\n",
    "    }\n",
    "\n",
    "    ctx.set(\"parsed_agent_data\", parsed_agent_data, scope=ContextScope.CHAIN)\n",
    "\n",
    "    return {\"parsed_agent_data\": parsed_agent_data}\n",
    "\n",
    "\n",
    "@forge.step(\n",
    "    name=\"build_prompts\",\n",
    "    deps=[\"parse_agent_data\"],\n",
    "    produces=[\"prompts\"],\n",
    "    description=\"Build LLM prompts for metrics and analysis\",\n",
    "    group=\"response_builder\"\n",
    ")\n",
    "async def build_prompts(ctx: ChainContext) -> Dict:\n",
    "    \"\"\"\n",
    "    Build LLM prompts for financial metrics and strategic analysis.\n",
    "    \n",
    "    Creates two prompts:\n",
    "    1. Financial metrics prompt (for extracting numbers)\n",
    "    2. Strategic analysis prompt (for SWOT, thesis, etc.)\n",
    "    \n",
    "    Equivalent to: ResponseBuilderAndGenerator.prompt_builder\n",
    "    \"\"\"\n",
    "    parsed_data = ctx.get(\"parsed_agent_data\", {})\n",
    "    context_output = ctx.get(\"context_builder_output\", {})\n",
    "    prioritization = ctx.get(\"content_prioritization_output\", {})\n",
    "\n",
    "    company_name = context_output.get(\"company_name\", \"Unknown Company\")\n",
    "    source_priorities = prioritization.get(\"temporal_source_prioritizer\", {})\n",
    "\n",
    "    # Build financial metrics prompt\n",
    "    financial_metrics_prompt = f\"\"\"\n",
    "## Task: Extract Financial Metrics for {company_name}\n",
    "\n",
    "[SEC_AGENT]\n",
    "{parsed_data.get(ToolName.SEC_TOOL.value, 'No SEC data available')}\n",
    "\n",
    "[EARNINGS_AGENT]\n",
    "{parsed_data.get(ToolName.EARNINGS_TOOL.value, 'No earnings data available')}\n",
    "\n",
    "[NEWS_AGENT]\n",
    "{parsed_data.get(ToolName.NEWS_TOOL.value, 'No news data available')}\n",
    "\n",
    "Extract precise financial metrics with citations for each value.\n",
    "\"\"\"\n",
    "\n",
    "    # Build strategic analysis prompt with source weights\n",
    "    news_pct = source_priorities.get(ToolName.NEWS_TOOL.value, 40)\n",
    "    earnings_pct = source_priorities.get(ToolName.EARNINGS_TOOL.value, 30)\n",
    "    sec_pct = source_priorities.get(ToolName.SEC_TOOL.value, 30)\n",
    "\n",
    "    strategic_analysis_prompt = f\"\"\"\n",
    "## Task: Strategic Analysis for {company_name}\n",
    "\n",
    "[NEWS_AGENT] (Include {news_pct}% from this source)\n",
    "{parsed_data.get(ToolName.NEWS_TOOL.value, 'No news data available')}\n",
    "\n",
    "[EARNINGS_AGENT] (Include {earnings_pct}% from this source)\n",
    "{parsed_data.get(ToolName.EARNINGS_TOOL.value, 'No earnings data available')}\n",
    "\n",
    "[SEC_AGENT] (Include {sec_pct}% from this source)\n",
    "{parsed_data.get(ToolName.SEC_TOOL.value, 'No SEC data available')}\n",
    "\n",
    "Generate comprehensive strategic analysis including SWOT, investment thesis, and recent developments.\n",
    "\"\"\"\n",
    "\n",
    "    prompts = {\n",
    "        \"financial_metrics_prompt\": financial_metrics_prompt,\n",
    "        \"strategic_analysis_prompt\": strategic_analysis_prompt,\n",
    "    }\n",
    "\n",
    "    ctx.set(\"prompts\", prompts, scope=ContextScope.CHAIN)\n",
    "\n",
    "    return {\"prompts\": prompts}\n",
    "\n",
    "\n",
    "print(\"\u2705 Response builder parsing steps registered!\")\n",
    "print(f\"   - parse_agent_data: Formats raw data\")\n",
    "print(f\"   - build_prompts: Creates LLM prompts\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\n",
    "#                       LLM GENERATION STEPS (PARALLEL)\n",
    "# \u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\n",
    "\n",
    "@forge.step(\n",
    "    name=\"generate_financial_metrics\",\n",
    "    deps=[\"build_prompts\"],\n",
    "    produces=[\"financial_metrics\"],\n",
    "    description=\"Generate financial metrics using LLM\",\n",
    "    timeout_ms=120000,  # 2 minute timeout for LLM calls\n",
    "    group=\"response_builder\"\n",
    ")\n",
    "async def generate_financial_metrics(ctx: ChainContext) -> Dict:\n",
    "    \"\"\"\n",
    "    Generate financial metrics using structured LLM output.\n",
    "    \n",
    "    This step runs in PARALLEL with generate_strategic_analysis.\n",
    "    \n",
    "    In production, this would:\n",
    "    1. Call the LLM gateway with the prompt\n",
    "    2. Use structured output (JSON mode)\n",
    "    3. Parse into FinancialMetricsResponse\n",
    "    \n",
    "    Equivalent to: ResponseBuilderAndGenerator.get_structured_response\n",
    "    \"\"\"\n",
    "    prompts = ctx.get(\"prompts\", {})\n",
    "    prompt = prompts.get(\"financial_metrics_prompt\", \"\")\n",
    "\n",
    "    # In production:\n",
    "    # from flowforge.services.llm_gateway import get_llm_client\n",
    "    # client = get_llm_client()\n",
    "    # response = await client.generate_async(\n",
    "    #     prompt,\n",
    "    #     system_prompt=FINANCIAL_METRICS_SYSTEM_PROMPT,\n",
    "    #     response_format=FinancialMetricsResponse\n",
    "    # )\n",
    "\n",
    "    # Placeholder response\n",
    "    financial_metrics = {\n",
    "        \"current_annual_revenue\": None,\n",
    "        \"ebitda_margin\": None,\n",
    "        \"stock_price\": None,\n",
    "        \"market_cap\": None,\n",
    "        \"revenue_growth_trajectory\": None,\n",
    "    }\n",
    "\n",
    "    ctx.set(\"financial_metrics\", financial_metrics, scope=ContextScope.CHAIN)\n",
    "\n",
    "    return {\"financial_metrics\": financial_metrics}\n",
    "\n",
    "\n",
    "@forge.step(\n",
    "    name=\"generate_strategic_analysis\",\n",
    "    deps=[\"build_prompts\"],  # Same dep = parallel with metrics!\n",
    "    produces=[\"strategic_analysis\"],\n",
    "    description=\"Generate strategic analysis using LLM\",\n",
    "    timeout_ms=120000,\n",
    "    group=\"response_builder\"\n",
    ")\n",
    "async def generate_strategic_analysis(ctx: ChainContext) -> Dict:\n",
    "    \"\"\"\n",
    "    Generate strategic analysis using structured LLM output.\n",
    "    \n",
    "    Runs in PARALLEL with generate_financial_metrics.\n",
    "    \"\"\"\n",
    "    prompts = ctx.get(\"prompts\", {})\n",
    "    prompt = prompts.get(\"strategic_analysis_prompt\", \"\")\n",
    "\n",
    "    # Placeholder response\n",
    "    strategic_analysis = {\n",
    "        \"strength\": [],\n",
    "        \"weakness\": [],\n",
    "        \"opportunity\": [],\n",
    "        \"threat\": [],\n",
    "        \"investment_thesis\": [],\n",
    "        \"key_risk_highlights\": [],\n",
    "        \"strategic_opportunities\": [],\n",
    "        \"recent_developments\": [],\n",
    "        \"sources\": [],\n",
    "    }\n",
    "\n",
    "    ctx.set(\"strategic_analysis\", strategic_analysis, scope=ContextScope.CHAIN)\n",
    "\n",
    "    return {\"strategic_analysis\": strategic_analysis}\n",
    "\n",
    "\n",
    "@forge.step(\n",
    "    name=\"validate_metrics\",\n",
    "    deps=[\"generate_financial_metrics\"],\n",
    "    produces=[\"validation_results\"],\n",
    "    description=\"Validate extracted financial metrics against sources\",\n",
    "    group=\"response_builder\"\n",
    ")\n",
    "async def validate_metrics(ctx: ChainContext) -> Dict:\n",
    "    \"\"\"\n",
    "    Validate extracted financial metrics against source documents.\n",
    "    \n",
    "    This step:\n",
    "    1. Checks that all citations are valid\n",
    "    2. Verifies quotes exist in source documents\n",
    "    3. Runs sanity checks on numeric values\n",
    "    \n",
    "    Equivalent to: MetricsValidator.validate_financial_metrics\n",
    "    \"\"\"\n",
    "    financial_metrics = ctx.get(\"financial_metrics\", {})\n",
    "    parsed_data = ctx.get(\"parsed_agent_data\", {})\n",
    "\n",
    "    # In production, use MetricsValidator:\n",
    "    # validation_results = MetricsValidator.validate_financial_metrics(\n",
    "    #     FinancialMetricsResponse(**financial_metrics),\n",
    "    #     parsed_data\n",
    "    # )\n",
    "\n",
    "    # Placeholder validation\n",
    "    validation_results = {\n",
    "        \"validation_summary\": {\n",
    "            \"total_fields_checked\": 0,\n",
    "            \"fields_with_values\": 0,\n",
    "            \"sources_verified\": 0,\n",
    "            \"warnings_count\": 0,\n",
    "        },\n",
    "        \"field_validations\": {},\n",
    "        \"warnings\": [],\n",
    "        \"sanity_checks\": {\"passed\": [], \"failed\": [], \"warnings\": []}\n",
    "    }\n",
    "\n",
    "    ctx.set(\"validation_results\", validation_results, scope=ContextScope.CHAIN)\n",
    "\n",
    "    return {\"validation_results\": validation_results}\n",
    "\n",
    "\n",
    "@forge.step(\n",
    "    name=\"build_response\",\n",
    "    deps=[\"generate_financial_metrics\", \"generate_strategic_analysis\", \"validate_metrics\"],\n",
    "    produces=[\"final_response\"],\n",
    "    description=\"Build final CMPT response\",\n",
    "    group=\"response_builder\"\n",
    ")\n",
    "async def build_response(ctx: ChainContext) -> Dict:\n",
    "    \"\"\"\n",
    "    Build the final CMPT response combining all outputs.\n",
    "    \n",
    "    This is the final step that assembles everything into\n",
    "    the ChainServerResponse format.\n",
    "    \n",
    "    Equivalent to: ChainOrchestrator.execute_chain (final assembly)\n",
    "    \"\"\"\n",
    "    context_output = ctx.get(\"context_builder_output\", {})\n",
    "    prioritization = ctx.get(\"content_prioritization_output\", {})\n",
    "    financial_metrics = ctx.get(\"financial_metrics\", {})\n",
    "    strategic_analysis = ctx.get(\"strategic_analysis\", {})\n",
    "    validation_results = ctx.get(\"validation_results\", {})\n",
    "    parsed_data = ctx.get(\"parsed_agent_data\", {})\n",
    "\n",
    "    final_response = {\n",
    "        \"context_builder\": {\n",
    "            \"company_info\": context_output.get(\"company_info\"),\n",
    "            \"temporal_context\": context_output.get(\"temporal_context\"),\n",
    "            \"meeting_date\": context_output.get(\"meeting_date\"),\n",
    "            \"rbc_persona\": context_output.get(\"rbc_persona\"),\n",
    "            \"corporate_client_persona\": context_output.get(\"corporate_client_persona\"),\n",
    "        },\n",
    "        \"content_prioritization\": {\n",
    "            \"temporal_source_prioritizer\": prioritization.get(\"temporal_source_prioritizer\"),\n",
    "            \"subqueries_from_engine\": prioritization.get(\"subqueries_from_engine\"),\n",
    "            \"topic_ranker_result\": prioritization.get(\"topic_ranker_result\"),\n",
    "        },\n",
    "        \"response_builder_and_generator\": {\n",
    "            \"financial_metrics_result\": financial_metrics,\n",
    "            \"strategic_analysis_result\": strategic_analysis,\n",
    "            \"validation_results\": validation_results,\n",
    "            \"parsed_data_agent_chunks\": {k: len(v) for k, v in parsed_data.items()},\n",
    "            \"company_name\": context_output.get(\"company_name\"),\n",
    "        },\n",
    "    }\n",
    "\n",
    "    ctx.set(\"final_response\", final_response, scope=ContextScope.CHAIN)\n",
    "\n",
    "    return {\"final_response\": final_response}\n",
    "\n",
    "\n",
    "print(\"\u2705 LLM generation and final steps registered!\")\n",
    "print(f\"   - generate_financial_metrics || generate_strategic_analysis (parallel)\")\n",
    "print(f\"   - validate_metrics: Verifies citations\")\n",
    "print(f\"   - build_response: Final assembly\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"10-chain-definition\"></a>\n",
    "## 10. Creating the Chain Definition\n",
    "\n",
    "Now we define the chain that ties all the steps together. The `@forge.chain()` decorator registers the chain with FlowForge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\n",
    "#                           CHAIN DEFINITION\n",
    "# \u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\n",
    "\n",
    "@forge.chain(name=\"cmpt_pipeline\", description=\"Client Meeting Prep Tool pipeline\")\n",
    "class CMPTPipeline:\n",
    "    \"\"\"\n",
    "    Complete CMPT pipeline chain.\n",
    "    \n",
    "    This class defines the chain by listing all steps in execution order.\n",
    "    FlowForge will:\n",
    "    1. Resolve dependencies from step definitions\n",
    "    2. Build the DAG\n",
    "    3. Execute steps with automatic parallelization\n",
    "    \n",
    "    Execution Flow:\n",
    "    ================\n",
    "    \n",
    "    1. Context Builder:\n",
    "       extract_company_info \u2192 extract_temporal_context \u2192 build_context\n",
    "       \n",
    "    2. Content Prioritization (PARALLEL):\n",
    "       temporal_source_prioritizer \u2510\n",
    "                                   \u251c\u2192 prioritize_content\n",
    "       generate_subqueries        \u2518\n",
    "       \n",
    "    3. Data Fetch (PARALLEL):\n",
    "       fetch_news_data    \u2510\n",
    "       fetch_sec_data     \u251c\u2192 parse_agent_data\n",
    "       fetch_earnings_data\u2518\n",
    "       \n",
    "    4. Response Builder:\n",
    "       parse_agent_data \u2192 build_prompts\n",
    "       \n",
    "    5. LLM Generation (PARALLEL):\n",
    "       generate_financial_metrics  \u2510\n",
    "                                   \u251c\u2192 build_response\n",
    "       generate_strategic_analysis \u2518\n",
    "       validate_metrics            \u2518\n",
    "    \"\"\"\n",
    "\n",
    "    steps = [\n",
    "        # Context Builder (sequential)\n",
    "        \"extract_company_info\",\n",
    "        \"extract_temporal_context\",\n",
    "        \"build_context\",\n",
    "        \n",
    "        # Content Prioritization (prioritizer & subqueries run in parallel)\n",
    "        \"temporal_source_prioritizer\",\n",
    "        \"generate_subqueries\",\n",
    "        \"prioritize_content\",\n",
    "        \n",
    "        # Data Fetch (all three run in parallel)\n",
    "        \"fetch_news_data\",\n",
    "        \"fetch_sec_data\",\n",
    "        \"fetch_earnings_data\",\n",
    "        \n",
    "        # Response Builder\n",
    "        \"parse_agent_data\",\n",
    "        \"build_prompts\",\n",
    "        \n",
    "        # LLM Generation (metrics & analysis run in parallel)\n",
    "        \"generate_financial_metrics\",\n",
    "        \"generate_strategic_analysis\",\n",
    "        \n",
    "        # Validation & Final Response\n",
    "        \"validate_metrics\",\n",
    "        \"build_response\",\n",
    "    ]\n",
    "\n",
    "    # Error handling mode:\n",
    "    # - \"fail_fast\": Stop on first error\n",
    "    # - \"continue\": Try to continue with other steps\n",
    "    error_handling = \"continue\"\n",
    "\n",
    "\n",
    "print(\"\u2705 Chain definition registered!\")\n",
    "print(f\"   - Chain name: cmpt_pipeline\")\n",
    "print(f\"   - Total steps: {len(CMPTPipeline.steps)}\")\n",
    "print(f\"   - Error handling: {CMPTPipeline.error_handling}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"16-validation\"></a>\n",
    "## 16. Chain Validation\n",
    "\n",
    "FlowForge provides built-in validation to check:\n",
    "- All steps exist\n",
    "- Dependencies are resolvable\n",
    "- No circular dependencies\n",
    "- Chain structure is valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\n",
    "#                           CHAIN VALIDATION\n",
    "# \u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"  CMPT Chain Validation\")\n",
    "print(\"=\" * 60 + \"\\n\")\n",
    "\n",
    "# Validate the chain\n",
    "validation_result = forge.check(\"cmpt_pipeline\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"  Validation Summary\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\\nValid: {validation_result['valid']}\")\n",
    "print(f\"Errors: {len(validation_result.get('errors', []))}\")\n",
    "print(f\"Warnings: {len(validation_result.get('warnings', []))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"17-visualization\"></a>\n",
    "## 17. DAG Visualization\n",
    "\n",
    "FlowForge can generate visual representations of the chain DAG in:\n",
    "- ASCII art (for terminal)\n",
    "- Mermaid.js (for documentation/web)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\n",
    "#                           DAG VISUALIZATION\n",
    "# \u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"  CMPT Chain DAG (ASCII)\")\n",
    "print(\"=\" * 60 + \"\\n\")\n",
    "\n",
    "# Generate ASCII visualization\n",
    "forge.graph(\"cmpt_pipeline\", format=\"ascii\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mermaid visualization (can be rendered in Jupyter or documentation)\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"  CMPT Chain DAG (Mermaid)\")\n",
    "print(\"=\" * 60 + \"\\n\")\n",
    "\n",
    "mermaid_output = forge.graph(\"cmpt_pipeline\", format=\"mermaid\")\n",
    "\n",
    "# If you have mermaid rendering enabled in Jupyter:\n",
    "# from IPython.display import display, Markdown\n",
    "# display(Markdown(f\"```mermaid\\n{mermaid_output}\\n```\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"18-execution\"></a>\n",
    "## 18. Running the Chain\n",
    "\n",
    "Now let's execute the chain with sample data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\n",
    "#                           CHAIN EXECUTION\n",
    "# \u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\n",
    "\n",
    "async def run_cmpt_chain(\n",
    "    company_name: str,\n",
    "    meeting_date: Optional[str] = None,\n",
    "    client_email: Optional[str] = None,\n",
    "    verbose: bool = False,\n",
    ") -> Dict:\n",
    "    \"\"\"\n",
    "    Convenience function to run the CMPT chain.\n",
    "    \n",
    "    Args:\n",
    "        company_name: Name of the company to analyze\n",
    "        meeting_date: Meeting date (YYYY-MM-DD format)\n",
    "        client_email: Client email address\n",
    "        verbose: Enable verbose logging\n",
    "    \n",
    "    Returns:\n",
    "        Final CMPT response dictionary\n",
    "    \"\"\"\n",
    "    if verbose:\n",
    "        logging.getLogger().setLevel(logging.DEBUG)\n",
    "\n",
    "    data = {\n",
    "        \"company_name\": company_name,\n",
    "        \"meeting_date\": meeting_date or datetime.now().strftime(\"%Y-%m-%d\"),\n",
    "        \"client_email\": client_email,\n",
    "    }\n",
    "\n",
    "    result = await forge.launch(\"cmpt_pipeline\", data=data)\n",
    "    return result\n",
    "\n",
    "\n",
    "# Execute the chain\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"  Running CMPT Chain\")\n",
    "print(\"=\" * 60 + \"\\n\")\n",
    "\n",
    "result = await run_cmpt_chain(\n",
    "    company_name=\"Apple Inc.\",\n",
    "    meeting_date=\"2025-01-15\",\n",
    "    verbose=False,\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"  Execution Results\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\\nSuccess: {result.get('success', 'N/A')}\")\n",
    "print(f\"Steps completed: {len(result.get('step_results', []))}\")\n",
    "\n",
    "if result.get('context'):\n",
    "    ctx = result['context']\n",
    "    print(f\"Context keys: {list(ctx.keys())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"19-error-handling\"></a>\n",
    "## 19. Error Handling & Debugging\n",
    "\n",
    "FlowForge provides several mechanisms for error handling:\n",
    "- `error_handling=\"fail_fast\"`: Stop on first error\n",
    "- `error_handling=\"continue\"`: Try to continue with other steps\n",
    "- `debug_callback`: Called after each step for debugging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\n",
    "#                       ERROR HANDLING & DEBUGGING\n",
    "# \u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\n",
    "\n",
    "# Debug callback example\n",
    "def debug_callback(ctx, step_name, result):\n",
    "    \"\"\"\n",
    "    Called after each step completes.\n",
    "    Useful for monitoring execution and debugging.\n",
    "    \"\"\"\n",
    "    success = result.get('success', True)\n",
    "    duration = result.get('duration_ms', 0)\n",
    "    status = \"\u2713\" if success else \"\u2717\"\n",
    "    print(f\"  {status} {step_name}: {duration:.0f}ms\")\n",
    "\n",
    "\n",
    "# Run with debug callback\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"  Running with Debug Callback\")\n",
    "print(\"=\" * 60 + \"\\n\")\n",
    "\n",
    "result = await forge.launch(\n",
    "    \"cmpt_pipeline\",\n",
    "    data={\"company_name\": \"Microsoft\", \"meeting_date\": \"2025-02-01\"},\n",
    "    debug_callback=debug_callback\n",
    ")\n",
    "\n",
    "print(f\"\\nFinal result: {'Success' if result.get('success') else 'Failed'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"20-middleware\"></a>\n",
    "## 20. Middleware Integration (Advanced)\n",
    "\n",
    "FlowForge supports middleware for cross-cutting concerns like logging, caching, and metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\n",
    "#                           MIDDLEWARE EXAMPLE\n",
    "# \u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\n",
    "\n",
    "# Example: Timing middleware\n",
    "class TimingMiddleware:\n",
    "    \"\"\"\n",
    "    Middleware that tracks execution time of each step.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.timings = {}\n",
    "    \n",
    "    async def before(self, ctx, step_name):\n",
    "        \"\"\"Called before step execution\"\"\"\n",
    "        import time\n",
    "        ctx.set(f\"_timing_{step_name}_start\", time.time())\n",
    "    \n",
    "    async def after(self, ctx, step_name, result):\n",
    "        \"\"\"Called after step execution\"\"\"\n",
    "        import time\n",
    "        start = ctx.get(f\"_timing_{step_name}_start\")\n",
    "        if start:\n",
    "            duration = time.time() - start\n",
    "            self.timings[step_name] = duration\n",
    "            logger.info(f\"Step {step_name} took {duration:.2f}s\")\n",
    "\n",
    "\n",
    "# Register middleware\n",
    "# timing_middleware = TimingMiddleware()\n",
    "# forge.use(timing_middleware)\n",
    "\n",
    "print(\"\u2705 Middleware example defined!\")\n",
    "print(\"   Note: In production, register with forge.use(middleware)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"21-resources\"></a>\n",
    "## 21. Additional Components from Old Code\n",
    "\n",
    "The following sections document additional components from the original CMPT implementation that can be integrated as needed.\n",
    "\n",
    "---\n",
    "\n",
    "### 21.1 Metrics Validator (validation_utils.py)\n",
    "\n",
    "The MetricsValidator validates extracted financial metrics against source documents to ensure LLM outputs are grounded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\n",
    "#                    METRICS VALIDATOR (From validation_utils.py)\n",
    "# \u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\n",
    "\n",
    "class MetricsValidator:\n",
    "    \"\"\"\n",
    "    Validates extracted financial metrics against source chunks.\n",
    "    \n",
    "    This is CRITICAL for production - ensures that LLM extractions are \n",
    "    actually grounded in the source documents and not hallucinated.\n",
    "    \n",
    "    Key Validations:\n",
    "    1. Citation has required fields (source_agent, source_content, reasoning)\n",
    "    2. Quoted text actually exists in source documents\n",
    "    3. Referenced agent names are valid\n",
    "    4. Numeric values pass sanity checks\n",
    "    \"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def validate_financial_metrics(\n",
    "        metrics,  # FinancialMetricsResponse object\n",
    "        source_chunks: Dict[str, str]\n",
    "    ) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Validate extracted financial metrics against source chunks.\n",
    "        \n",
    "        Args:\n",
    "            metrics: FinancialMetricsResponse object with extracted data\n",
    "            source_chunks: Dictionary with agent names as keys and combined content as values\n",
    "                          e.g., {\"news_agent\": \"...\", \"earnings_agent\": \"...\", \"SEC_agent\": \"...\"}\n",
    "        \n",
    "        Returns:\n",
    "            Dictionary with validation results\n",
    "        \"\"\"\n",
    "        validation_results = {\n",
    "            \"validation_summary\": {\n",
    "                \"total_fields_checked\": 0,\n",
    "                \"fields_with_values\": 0,\n",
    "                \"fields_with_sources\": 0,\n",
    "                \"sources_verified\": 0,\n",
    "                \"sources_not_found\": 0,\n",
    "                \"warnings_count\": 0\n",
    "            },\n",
    "            \"field_validations\": {},\n",
    "            \"warnings\": []\n",
    "        }\n",
    "        \n",
    "        # Critical fields that require source validation\n",
    "        critical_fields = [\n",
    "            (\"current_annual_revenue\", \"current_annual_revenue_citation\"),\n",
    "            (\"current_annual_revenue_yoy_change\", \"current_annual_revenue_yoy_change_citation\"),\n",
    "            (\"estimated_annual_revenue_next_year\", \"estimated_annual_revenue_next_year_citation\"),\n",
    "            (\"ebitda_margin\", \"ebitda_margin_citation\"),\n",
    "            (\"ebitda_margin_yoy_change\", \"ebitda_margin_yoy_change_citation\"),\n",
    "            (\"stock_price\", \"stock_price_citation\"),\n",
    "            (\"stock_price_yoy_change\", \"stock_price_yoy_change_citation\"),\n",
    "            (\"market_cap\", \"market_cap_citation\"),\n",
    "            (\"revenue_growth_trajectory\", \"revenue_growth_trajectory_citation\")\n",
    "        ]\n",
    "        \n",
    "        for value_field, citation_field in critical_fields:\n",
    "            validation_results[\"validation_summary\"][\"total_fields_checked\"] += 1\n",
    "            \n",
    "            field_value = getattr(metrics, value_field, None) if hasattr(metrics, value_field) else metrics.get(value_field)\n",
    "            citation = getattr(metrics, citation_field, None) if hasattr(metrics, citation_field) else metrics.get(citation_field)\n",
    "            \n",
    "            field_validation = {\n",
    "                \"has_value\": field_value is not None,\n",
    "                \"has_citation\": citation is not None,\n",
    "                \"citation_verified\": False,\n",
    "                \"extracted_numbers\": [],\n",
    "                \"issues\": []\n",
    "            }\n",
    "            \n",
    "            if field_value is not None:\n",
    "                validation_results[\"validation_summary\"][\"fields_with_values\"] += 1\n",
    "            \n",
    "            if citation is not None:\n",
    "                validation_results[\"validation_summary\"][\"fields_with_sources\"] += 1\n",
    "                \n",
    "                # Extract citation data\n",
    "                if isinstance(citation, dict):\n",
    "                    source_agents = citation.get('source_agent', [])\n",
    "                    source_contents = citation.get('source_content', [])\n",
    "                    reasoning = citation.get('reasoning', '')\n",
    "                elif hasattr(citation, 'source_agent'):\n",
    "                    source_agents = citation.source_agent\n",
    "                    source_contents = citation.source_content\n",
    "                    reasoning = citation.reasoning\n",
    "                else:\n",
    "                    field_validation[\"issues\"].append(\"Invalid citation format\")\n",
    "                    validation_results[\"field_validations\"][value_field] = field_validation\n",
    "                    continue\n",
    "                \n",
    "                # Validate citation completeness\n",
    "                if not source_agents or not source_contents or not reasoning:\n",
    "                    field_validation[\"issues\"].append(\"Citation missing required fields\")\n",
    "                else:\n",
    "                    # Verify quotes exist in source documents\n",
    "                    verified_quotes = 0\n",
    "                    for i, quote in enumerate(source_contents):\n",
    "                        agent_name = source_agents[i] if i < len(source_agents) else source_agents[0]\n",
    "                        if agent_name in source_chunks:\n",
    "                            if MetricsValidator._verify_citation_in_sources(quote, source_chunks[agent_name]):\n",
    "                                verified_quotes += 1\n",
    "                            else:\n",
    "                                field_validation[\"issues\"].append(f\"Quote #{i+1} not found in {agent_name}\")\n",
    "                    \n",
    "                    if verified_quotes == len(source_contents) and verified_quotes > 0:\n",
    "                        validation_results[\"validation_summary\"][\"sources_verified\"] += 1\n",
    "                        field_validation[\"citation_verified\"] = True\n",
    "                    else:\n",
    "                        validation_results[\"validation_summary\"][\"sources_not_found\"] += 1\n",
    "            \n",
    "            validation_results[\"field_validations\"][value_field] = field_validation\n",
    "        \n",
    "        # Run sanity checks\n",
    "        validation_results[\"sanity_checks\"] = MetricsValidator._run_sanity_checks(metrics)\n",
    "        validation_results[\"validation_summary\"][\"warnings_count\"] = len(validation_results[\"warnings\"])\n",
    "        \n",
    "        return validation_results\n",
    "    \n",
    "    @staticmethod\n",
    "    def _verify_citation_in_sources(citation: str, sources: str) -> bool:\n",
    "        \"\"\"Check if citation exists in sources with fuzzy matching AND exact number matching\"\"\"\n",
    "        if not citation or not sources:\n",
    "            return False\n",
    "        \n",
    "        # Extract numbers from both\n",
    "        citation_numbers = MetricsValidator._extract_numbers_from_text(citation)\n",
    "        sources_numbers = MetricsValidator._extract_numbers_from_text(sources)\n",
    "        \n",
    "        # All citation numbers must exist in sources\n",
    "        if citation_numbers:\n",
    "            for num in citation_numbers:\n",
    "                if not any(abs(num - src_num) < 0.01 for src_num in sources_numbers):\n",
    "                    return False\n",
    "        \n",
    "        # Fuzzy text match (70% threshold)\n",
    "        citation_clean = ' '.join(citation.lower().strip().split())\n",
    "        sources_clean = ' '.join(sources.lower().split())\n",
    "        \n",
    "        min_match_length = int(len(citation_clean) * 0.70)\n",
    "        for i in range(len(citation_clean) - min_match_length + 1):\n",
    "            substring = citation_clean[i:i + min_match_length]\n",
    "            if substring in sources_clean:\n",
    "                return True\n",
    "        \n",
    "        # Word overlap fallback (75% threshold)\n",
    "        citation_words = set(citation_clean.split())\n",
    "        sources_words = set(sources_clean.split())\n",
    "        if len(citation_words) > 0:\n",
    "            overlap = len(citation_words & sources_words) / len(citation_words)\n",
    "            if overlap >= 0.75:\n",
    "                return True\n",
    "        \n",
    "        return False\n",
    "    \n",
    "    @staticmethod\n",
    "    def _extract_numbers_from_text(text: str) -> List[float]:\n",
    "        \"\"\"Extract all numbers from text\"\"\"\n",
    "        pattern = r'\\$?\\d+(?:,\\d{3})*(?:\\.\\d+)?'\n",
    "        matches = re.findall(pattern, text)\n",
    "        numbers = []\n",
    "        for match in matches:\n",
    "            try:\n",
    "                clean_number = match.replace('$', '').replace(',', '')\n",
    "                numbers.append(float(clean_number))\n",
    "            except ValueError:\n",
    "                continue\n",
    "        return numbers\n",
    "    \n",
    "    @staticmethod\n",
    "    def _run_sanity_checks(metrics) -> Dict[str, Any]:\n",
    "        \"\"\"Run sanity checks on extracted metrics\"\"\"\n",
    "        checks = {\"passed\": [], \"failed\": [], \"warnings\": []}\n",
    "        \n",
    "        # Get metric value (works with dict or object)\n",
    "        def get_val(name):\n",
    "            return getattr(metrics, name, None) if hasattr(metrics, name) else metrics.get(name)\n",
    "        \n",
    "        # Revenue should be positive\n",
    "        revenue = get_val(\"current_annual_revenue\")\n",
    "        if revenue is not None:\n",
    "            if revenue > 0:\n",
    "                checks[\"passed\"].append(\"Revenue is positive\")\n",
    "            else:\n",
    "                checks[\"failed\"].append(f\"Revenue is non-positive: {revenue}\")\n",
    "        \n",
    "        # EBITDA margin should be between -100 and 100\n",
    "        ebitda = get_val(\"ebitda_margin\")\n",
    "        if ebitda is not None:\n",
    "            if -100 <= ebitda <= 100:\n",
    "                checks[\"passed\"].append(\"EBITDA margin in reasonable range\")\n",
    "            else:\n",
    "                checks[\"failed\"].append(f\"EBITDA margin out of range: {ebitda}%\")\n",
    "        \n",
    "        # Stock price should be positive\n",
    "        stock = get_val(\"stock_price\")\n",
    "        if stock is not None:\n",
    "            if stock > 0:\n",
    "                checks[\"passed\"].append(\"Stock price is positive\")\n",
    "            else:\n",
    "                checks[\"failed\"].append(f\"Stock price is non-positive: {stock}\")\n",
    "        \n",
    "        return checks\n",
    "    \n",
    "    @staticmethod\n",
    "    def print_validation_report(validation_results: Dict[str, Any]) -> None:\n",
    "        \"\"\"Pretty print validation results\"\"\"\n",
    "        print(\"\\n\" + \"=\" * 80)\n",
    "        print(\"FINANCIAL METRICS VALIDATION REPORT\")\n",
    "        print(\"=\" * 80)\n",
    "        \n",
    "        summary = validation_results[\"validation_summary\"]\n",
    "        print(f\"\\n\ud83d\udcca SUMMARY:\")\n",
    "        print(f\"  Total fields checked: {summary['total_fields_checked']}\")\n",
    "        print(f\"  Fields with values: {summary['fields_with_values']}\")\n",
    "        print(f\"  Fields with source citations: {summary['fields_with_sources']}\")\n",
    "        print(f\"  Citations verified in sources: {summary['sources_verified']}\")\n",
    "        print(f\"  Citations NOT found: {summary['sources_not_found']}\")\n",
    "        \n",
    "        if validation_results[\"warnings\"]:\n",
    "            print(f\"\\n\u26a0\ufe0f WARNINGS ({len(validation_results['warnings'])}):\")\n",
    "            for warning in validation_results[\"warnings\"][:5]:\n",
    "                print(f\"  - {warning}\")\n",
    "        \n",
    "        sanity = validation_results[\"sanity_checks\"]\n",
    "        print(f\"\\n\u2713 SANITY CHECKS: {len(sanity['passed'])} passed, {len(sanity['failed'])} failed\")\n",
    "        print(\"=\" * 80 + \"\\n\")\n",
    "\n",
    "\n",
    "print(\"\u2705 MetricsValidator class defined!\")\n",
    "print(\"   - validate_financial_metrics(): Main validation method\")\n",
    "print(\"   - _verify_citation_in_sources(): Fuzzy quote matching\")\n",
    "print(\"   - _run_sanity_checks(): Numeric value sanity checks\")\n",
    "print(\"   - print_validation_report(): Pretty print results\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 21.2 Static Subquery Engine\n",
    "\n",
    "The Static Subquery Engine generates pre-defined subqueries for each data agent. This is used when you need consistent, structured queries rather than dynamic LLM-generated ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\n",
    "#                    STATIC SUBQUERY ENGINE (From static_subquery_engine.py)\n",
    "# \u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\n",
    "\n",
    "class StaticSubqueryEngine:\n",
    "    \"\"\"\n",
    "    Generates static, pre-defined subqueries for data agents.\n",
    "    \n",
    "    This provides consistent, structured queries that can be used when:\n",
    "    - You want deterministic query generation\n",
    "    - You don't need dynamic LLM-generated queries\n",
    "    - You want to ensure specific data coverage\n",
    "    \n",
    "    Subqueries are organized by agent type:\n",
    "    - SEC_agent: Financial statements, balance sheet queries\n",
    "    - news_agent: Recent news, executive changes, M&A\n",
    "    - earnings_agent: Earnings transcript queries\n",
    "    \"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def get_subquery_arguments(\n",
    "        company_name: str,\n",
    "        fiscal_year: str,\n",
    "        fiscal_quarter: str\n",
    "    ) -> Dict[str, List[Dict]]:\n",
    "        \"\"\"\n",
    "        Generate subquery arguments for all data agents.\n",
    "        \n",
    "        Args:\n",
    "            company_name: Company to query\n",
    "            fiscal_year: e.g., \"2025\"\n",
    "            fiscal_quarter: e.g., \"1\", \"2\", \"3\", \"4\"\n",
    "            \n",
    "        Returns:\n",
    "            Dictionary with agent names as keys and list of subqueries as values\n",
    "        \"\"\"\n",
    "        end_date = datetime.now().strftime('%Y-%m-%d')\n",
    "        month_ago = (datetime.now() - timedelta(days=31)).strftime('%Y-%m-%d')\n",
    "        five_days_ago = (datetime.now() - timedelta(days=5)).strftime('%Y-%m-%d')\n",
    "        year_ago = (datetime.now() - timedelta(days=365)).strftime('%Y-%m-%d')\n",
    "        \n",
    "        return {\n",
    "            # SEC Filing Subqueries\n",
    "            ToolName.SEC_TOOL.value: [\n",
    "                {\n",
    "                    \"reporting_entity\": company_name,\n",
    "                    \"search_queries\": [\n",
    "                        \"consolidated statements of operations\",\n",
    "                        \"statements of income\",\n",
    "                        \"total revenue\"\n",
    "                    ],\n",
    "                    \"keywords\": [\n",
    "                        \"net sales\", \"total revenue\", \"quarterly revenue\",\n",
    "                        \"three months ended\", \"revenue\", \"fiscal year\"\n",
    "                    ],\n",
    "                    \"get_latest\": 8,  # Last 8 quarters for trajectory\n",
    "                    \"description\": \"Income statement data for revenue metrics\"\n",
    "                },\n",
    "                {\n",
    "                    \"reporting_entity\": company_name,\n",
    "                    \"search_queries\": [\n",
    "                        \"consolidated balance sheets\",\n",
    "                        \"stockholders equity\",\n",
    "                        \"common stock outstanding shares\"\n",
    "                    ],\n",
    "                    \"keywords\": [\n",
    "                        \"outstanding shares\", \"common stock\", \"shares outstanding\",\n",
    "                        \"total stockholders equity\"\n",
    "                    ],\n",
    "                    \"get_latest\": 1,  # Latest for market cap calculation\n",
    "                    \"description\": \"Balance sheet for shares outstanding\"\n",
    "                },\n",
    "                {\n",
    "                    \"reporting_entity\": company_name,\n",
    "                    \"search_queries\": [\n",
    "                        \"operating income\",\n",
    "                        \"depreciation and amortization\",\n",
    "                        \"EBITDA\"\n",
    "                    ],\n",
    "                    \"keywords\": [\n",
    "                        \"operating income\", \"depreciation\", \"amortization\",\n",
    "                        \"D&A\", \"operating expenses\"\n",
    "                    ],\n",
    "                    \"get_latest\": 2,\n",
    "                    \"description\": \"Data for EBITDA margin calculation\"\n",
    "                }\n",
    "            ],\n",
    "            \n",
    "            # News Agent Subqueries\n",
    "            ToolName.NEWS_TOOL.value: [\n",
    "                {\n",
    "                    \"search_query\": f\"{company_name} executive leadership CEO CFO changes\",\n",
    "                    \"topics\": [\"executive\", \"CEO\", \"CFO\", \"leadership\", \"management\"],\n",
    "                    \"absolute_date_range\": {\"start_date\": month_ago, \"end_date\": end_date},\n",
    "                    \"description\": \"Executive and leadership changes\"\n",
    "                },\n",
    "                {\n",
    "                    \"search_query\": f\"{company_name} mergers acquisitions M&A deal\",\n",
    "                    \"topics\": [\"merger\", \"acquisition\", \"M&A\", \"deal\", \"buyout\"],\n",
    "                    \"absolute_date_range\": {\"start_date\": month_ago, \"end_date\": end_date},\n",
    "                    \"description\": \"M&A activity\"\n",
    "                },\n",
    "                {\n",
    "                    \"search_query\": f\"{company_name} stock price market cap valuation\",\n",
    "                    \"topics\": [\"market cap\", \"stock price\", \"valuation\", \"shares\"],\n",
    "                    \"absolute_date_range\": {\"start_date\": five_days_ago, \"end_date\": end_date},\n",
    "                    \"description\": \"Recent stock price and market cap\"\n",
    "                },\n",
    "                {\n",
    "                    \"search_query\": f\"{company_name} strategy growth expansion\",\n",
    "                    \"topics\": [\"strategy\", \"growth\", \"expansion\", \"investment\"],\n",
    "                    \"absolute_date_range\": {\"start_date\": month_ago, \"end_date\": end_date},\n",
    "                    \"description\": \"Strategic developments\"\n",
    "                },\n",
    "                {\n",
    "                    \"search_query\": f\"{company_name} risk regulatory lawsuit\",\n",
    "                    \"topics\": [\"risk\", \"regulatory\", \"lawsuit\", \"compliance\", \"SEC\"],\n",
    "                    \"absolute_date_range\": {\"start_date\": month_ago, \"end_date\": end_date},\n",
    "                    \"description\": \"Risk and regulatory news\"\n",
    "                }\n",
    "            ],\n",
    "            \n",
    "            # Earnings Agent Subqueries\n",
    "            ToolName.EARNINGS_TOOL.value: [\n",
    "                {\n",
    "                    \"company_name\": company_name,\n",
    "                    \"fiscal_year\": fiscal_year,\n",
    "                    \"fiscal_quarter\": fiscal_quarter,\n",
    "                    \"query\": f\"Give me the earnings transcript for {company_name} for fiscal year: {fiscal_year} and quarter: {fiscal_quarter}.\",\n",
    "                    \"description\": \"Latest earnings transcript\"\n",
    "                },\n",
    "                {\n",
    "                    \"company_name\": company_name,\n",
    "                    \"fiscal_year\": str(int(fiscal_year) - 1) if fiscal_quarter == \"1\" else fiscal_year,\n",
    "                    \"fiscal_quarter\": \"4\" if fiscal_quarter == \"1\" else str(int(fiscal_quarter) - 1),\n",
    "                    \"query\": f\"Give me the previous quarter earnings transcript for {company_name}.\",\n",
    "                    \"description\": \"Prior quarter for YoY comparison\"\n",
    "                }\n",
    "            ]\n",
    "        }\n",
    "    \n",
    "    @staticmethod\n",
    "    def get_subquery_count(subqueries: Dict[str, List[Dict]]) -> Dict[str, int]:\n",
    "        \"\"\"Get count of subqueries per agent\"\"\"\n",
    "        return {agent: len(queries) for agent, queries in subqueries.items()}\n",
    "\n",
    "\n",
    "# Example usage\n",
    "print(\"\u2705 StaticSubqueryEngine class defined!\")\n",
    "\n",
    "# Demo\n",
    "demo_subqueries = StaticSubqueryEngine.get_subquery_arguments(\"Apple Inc.\", \"2025\", \"1\")\n",
    "counts = StaticSubqueryEngine.get_subquery_count(demo_subqueries)\n",
    "print(f\"\\n\ud83d\udcca Subquery counts for 'Apple Inc.':\")\n",
    "for agent, count in counts.items():\n",
    "    print(f\"   - {agent}: {count} subqueries\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 21.3 LLM Prompts (From llm_prompts.py)\n",
    "\n",
    "These are the detailed system prompts used for LLM extraction. They provide extensive guidance on:\n",
    "- What to extract\n",
    "- How to format citations\n",
    "- Calculation methods for derived metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\n",
    "#                    LLM PROMPTS (From llm_prompts.py)\n",
    "# \u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\n",
    "\n",
    "# Data templates for prompts (filled with agent content)\n",
    "DATA_FOR_FINANCIAL_METRICS_PROMPT = \"\"\"\n",
    "[SEC_AGENT]\n",
    "Agent Guidance:\n",
    "- Primary source for all financial metrics\n",
    "- Extract precise numbers from financial statements\n",
    "- Look for year-over-year comparisons in financial statements\n",
    "{SEC_AGENT_CONTENT}\n",
    "\n",
    "[EARNINGS_AGENT]\n",
    "Agent Guidance:\n",
    "- Use for forward guidance and quarterly commentary\n",
    "- Extract YoY growth rates mentioned in management discussion\n",
    "{EARNINGS_AGENT_CONTENT}\n",
    "\n",
    "[NEWS_AGENT]\n",
    "Agent Guidance:\n",
    "- Extract current stock price and today's price movement if mentioned in recent articles\n",
    "- Look for daily price changes (e.g., \"stock up $1.39 or 0.56% today\")\n",
    "{NEWS_AGENT_CONTENT}\n",
    "\"\"\"\n",
    "\n",
    "DATA_FOR_STRATEGIC_ANALYSIS_PROMPT = \"\"\"\n",
    "[NEWS_AGENT]\n",
    "Agent Guidance:\n",
    "- Include {NEWS_percentage}% of the response from this agent chunks\n",
    "- Extract date, category, and source URL for each development\n",
    "{NEWS_AGENT_CONTENT}\n",
    "\n",
    "[EARNINGS_AGENT]\n",
    "Agent Guidance:\n",
    "- Include {EARNINGS_percentage}% of the response from this agent chunks\n",
    "{EARNINGS_AGENT_CONTENT}\n",
    "\n",
    "[SEC_AGENT]\n",
    "Agent Guidance:\n",
    "- Include {SEC_percentage}% of the response from this agent chunks\n",
    "{SEC_AGENT_CONTENT}\n",
    "\"\"\"\n",
    "\n",
    "# Main prompt for financial metrics extraction\n",
    "FINANCIAL_METRICS_SYSTEM_PROMPT = \"\"\"\n",
    "## Task: Extract Financial Metrics\n",
    "\n",
    "You are analyzing financial data for {COMPANY_NAME}. Extract precise financial metrics from SEC filings and earnings reports.\n",
    "\n",
    "## CRITICAL JSON FORMAT REQUIREMENT:\n",
    "**For ALL numeric fields (float type):**\n",
    "- Return actual numbers (e.g., 96.773, 25.5, 1.39) OR the JSON null value\n",
    "- NEVER return strings like \"<UNKNOWN>\", \"N/A\", \"null\", \"None\", or any text placeholders\n",
    "- If data is genuinely missing after exhausting all sources, return null (JSON null, not the string \"null\")\n",
    "\n",
    "## CRITICAL: Source Attribution (REQUIRED FOR VERIFICATION)\n",
    "For ALL numeric fields, you MUST provide citations as dictionaries with these keys:\n",
    "- **source_agent**: List of agent names (e.g., [\"SEC_agent\", \"earnings_agent\"])\n",
    "- **source_content**: List of VERBATIM quotes from those agents (COPY-PASTE EXACTLY - DO NOT PARAPHRASE)\n",
    "- **reasoning**: String explaining your extraction/calculation logic\n",
    "\n",
    "**IMPORTANT CITATION RULES:**\n",
    "1. **VERBATIM QUOTES ONLY**: Copy-paste the EXACT text from source documents into source_content\n",
    "2. **QUOTE LENGTH**: Each quote should be 20-100 words (enough context to verify)\n",
    "3. **SHOW YOUR WORK**: For calculated metrics (like EBITDA margin), include actual calculation in reasoning\n",
    "\n",
    "## Extraction Guidelines:\n",
    "\n",
    "**Revenue Metrics (REQUIRED):**\n",
    "- Current annual revenue: Most recent 10-K or annualized 10-Q\n",
    "- Year-over-year change: Calculate or extract YoY % change in revenue\n",
    "- Next year estimate: Company guidance or extrapolate from growth trends\n",
    "\n",
    "**EBITDA Margin (REQUIRED - CALCULATE IF NEEDED):**\n",
    "- First, search for explicit \"EBITDA\" in SEC_AGENT\n",
    "- If not found, calculate: (Operating Income + Depreciation + Amortization) / Revenue \u00d7 100\n",
    "- MUST show calculation in reasoning\n",
    "\n",
    "**Stock Price (REQUIRED - MULTIPLE SOURCES):**\n",
    "1. Check NEWS_AGENT for recent price mentions\n",
    "2. Check SEC filing cover pages for \"Class A Common Stock\" price\n",
    "3. Extract daily change in both dollars and percentage\n",
    "4. Extract year-over-year change\n",
    "\n",
    "**Market Cap (CALCULATE IF NEEDED):**\n",
    "- Formula: Stock Price \u00d7 Outstanding Shares\n",
    "- Find \"Outstanding Shares\" in SEC_AGENT\n",
    "\n",
    "**Revenue Growth Trajectory (REQUIRED):**\n",
    "- Build dictionary of last 7 quarterly revenues\n",
    "- Use fiscal quarter notation: \"Q1 FY2026\", \"Q3 FY2025\"\n",
    "\n",
    "## Data Source Strategy:\n",
    "- SEC_AGENT: Financial statements, balance sheet, cash flow\n",
    "- EARNINGS_AGENT: Management commentary, guidance, Q&A mentions\n",
    "- NEWS_AGENT: Recent price action, market cap references, daily stock movements\n",
    "\n",
    "**Your goal: Maximize data extraction with VERBATIM, VERIFIABLE citations.**\n",
    "\"\"\"\n",
    "\n",
    "# Main prompt for strategic analysis\n",
    "STRATEGIC_ANALYSIS_SYSTEM_PROMPT = \"\"\"\n",
    "## Task: Strategic Analysis for Client Meeting\n",
    "\n",
    "You are preparing a strategic briefing for {COMPANY_NAME} for an RBC Capital Markets client meeting.\n",
    "\n",
    "**CRITICAL: You must fill ALL fields in the response schema. Do not skip any fields.**\n",
    "\n",
    "## Required Output Structure (ALL FIELDS MANDATORY):\n",
    "\n",
    "### 1. SWOT Analysis (strength, weakness, opportunity, threat)\n",
    "- 4-6 bullets each\n",
    "- Each bullet: 15-25 words, specific, data-backed\n",
    "\n",
    "### 2. Investment Thesis (investment_thesis field - REQUIRED)\n",
    "**Format**: List of 3-4 dictionaries, each with:\n",
    "- Key: One subheading (e.g., \"Growth Drivers\", \"Competitive Moat\")\n",
    "- Value: List of 2-4 bullet points (15-30 words each)\n",
    "\n",
    "### 3. Key Risk Highlights (key_risk_highlights field)\n",
    "- 5-7 critical risks\n",
    "- Each bullet: 15-30 words with impact and timeline\n",
    "\n",
    "### 4. Strategic Opportunities (strategic_opportunities field - REQUIRED)\n",
    "**Format**: List of 3-4 dictionaries with:\n",
    "- Key: Category (e.g., \"M&A Advisory\", \"Capital Raising\")\n",
    "- Value: List of 2-3 specific opportunities (15-30 words each)\n",
    "\n",
    "### 5. Recent Developments (recent_developments field - REQUIRED)\n",
    "**Format**: List of 4-6 dictionaries with:\n",
    "- **category**: ONE of: \"News\", \"M&A\", \"Management\", \"Company\", \"Industry\"\n",
    "- **header**: 5-10 word title/summary\n",
    "- **date**: Date in format \"MMM DD YYYY\"\n",
    "- **description**: 20-40 words describing what happened\n",
    "- **source_url**: Full URL to source\n",
    "\n",
    "### 6. Sources (sources field)\n",
    "- 8-12 sources cited with URLs\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def build_financial_metrics_prompt(\n",
    "    company_name: str,\n",
    "    sec_content: str,\n",
    "    earnings_content: str,\n",
    "    news_content: str\n",
    ") -> str:\n",
    "    \"\"\"Build the complete financial metrics prompt\"\"\"\n",
    "    data_section = DATA_FOR_FINANCIAL_METRICS_PROMPT.format(\n",
    "        SEC_AGENT_CONTENT=sec_content or \"No SEC data available\",\n",
    "        EARNINGS_AGENT_CONTENT=earnings_content or \"No earnings data available\",\n",
    "        NEWS_AGENT_CONTENT=news_content or \"No news data available\"\n",
    "    )\n",
    "    \n",
    "    return FINANCIAL_METRICS_SYSTEM_PROMPT.format(\n",
    "        COMPANY_NAME=company_name\n",
    "    ) + \"\\n\\n## Source Data:\\n\" + data_section\n",
    "\n",
    "\n",
    "def build_strategic_analysis_prompt(\n",
    "    company_name: str,\n",
    "    news_content: str,\n",
    "    earnings_content: str,\n",
    "    sec_content: str,\n",
    "    news_pct: int = 40,\n",
    "    earnings_pct: int = 30,\n",
    "    sec_pct: int = 30\n",
    ") -> str:\n",
    "    \"\"\"Build the complete strategic analysis prompt\"\"\"\n",
    "    data_section = DATA_FOR_STRATEGIC_ANALYSIS_PROMPT.format(\n",
    "        NEWS_AGENT_CONTENT=news_content or \"No news data available\",\n",
    "        EARNINGS_AGENT_CONTENT=earnings_content or \"No earnings data available\",\n",
    "        SEC_AGENT_CONTENT=sec_content or \"No SEC data available\",\n",
    "        NEWS_percentage=news_pct,\n",
    "        EARNINGS_percentage=earnings_pct,\n",
    "        SEC_percentage=sec_pct\n",
    "    )\n",
    "    \n",
    "    return STRATEGIC_ANALYSIS_SYSTEM_PROMPT.format(\n",
    "        COMPANY_NAME=company_name\n",
    "    ) + \"\\n\\n## Source Data:\\n\" + data_section\n",
    "\n",
    "\n",
    "print(\"\u2705 LLM Prompts defined!\")\n",
    "print(\"   - DATA_FOR_FINANCIAL_METRICS_PROMPT: Data template\")\n",
    "print(\"   - DATA_FOR_STRATEGIC_ANALYSIS_PROMPT: Data template with source weights\")\n",
    "print(\"   - FINANCIAL_METRICS_SYSTEM_PROMPT: Full extraction instructions\")\n",
    "print(\"   - STRATEGIC_ANALYSIS_SYSTEM_PROMPT: Full SWOT/thesis instructions\")\n",
    "print(\"   - build_financial_metrics_prompt(): Helper to build complete prompt\")\n",
    "print(\"   - build_strategic_analysis_prompt(): Helper to build complete prompt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 21.4 Persona Extraction Services (LDAP & ZoomInfo)\n",
    "\n",
    "The original CMPT chain includes optional persona extraction for:\n",
    "- **RBC Persona**: Employee information from internal LDAP\n",
    "- **Corporate Client Persona**: Client information from ZoomInfo\n",
    "\n",
    "These are marked as \"deferred\" in the original architecture but can be enabled for enhanced personalization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\n",
    "#              PERSONA EXTRACTION SERVICES (LDAP & ZoomInfo)\n",
    "# \u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\n",
    "\n",
    "\"\"\"\n",
    "Persona Extraction: Optional components for enhanced personalization.\n",
    "\n",
    "These services can be integrated as additional steps in the Context Builder\n",
    "to provide persona information about meeting attendees.\n",
    "\n",
    "Architecture:\n",
    "- LDAPService: Looks up RBC employee info by email (internal)\n",
    "- ZoomInfoService: Looks up external client info (corporate clients)\n",
    "\n",
    "Both services follow the LookupServiceInterface pattern.\n",
    "\"\"\"\n",
    "\n",
    "# Base interface that both services implement\n",
    "class LookupServiceInterface:\n",
    "    \"\"\"Interface for persona lookup services\"\"\"\n",
    "    def lookup(self, identifiers: List[str]) -> List[Dict[str, Any]]:\n",
    "        \"\"\"Lookup profiles by identifiers (email, name, etc.)\"\"\"\n",
    "        raise NotImplementedError\n",
    "\n",
    "\n",
    "class LDAPService(LookupServiceInterface):\n",
    "    \"\"\"\n",
    "    LDAP Service for RBC employee persona extraction.\n",
    "    \n",
    "    Looks up employee information from internal LDAP directory.\n",
    "    \n",
    "    Usage:\n",
    "        ldap_service = LDAPService(LDAPEmailStrategy())\n",
    "        profiles = ldap_service.lookup([\"employee@rbc.com\"])\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, strategy=None):\n",
    "        \"\"\"\n",
    "        Initialize with a lookup strategy.\n",
    "        \n",
    "        Args:\n",
    "            strategy: Strategy for LDAP lookup (email, name, etc.)\n",
    "        \"\"\"\n",
    "        self.strategy = strategy\n",
    "    \n",
    "    def lookup(self, identifiers: List[str]) -> List[Dict[str, Any]]:\n",
    "        \"\"\"\n",
    "        Lookup employee profiles from LDAP.\n",
    "        \n",
    "        Returns:\n",
    "            List of employee profiles with:\n",
    "            - name: Full name\n",
    "            - email: Email address\n",
    "            - title: Job title\n",
    "            - department: Department name\n",
    "            - division: Division name\n",
    "            - manager: Manager name\n",
    "        \"\"\"\n",
    "        # Placeholder - in production this would connect to LDAP\n",
    "        # Example using ldap3 library:\n",
    "        # from ldap3 import Server, Connection, SUBTREE\n",
    "        # server = Server(os.getenv('LDAP_SERVER'))\n",
    "        # conn = Connection(server, user=os.getenv('LDAP_USER'), password=os.getenv('LDAP_PASS'))\n",
    "        # conn.search(base_dn, f'(mail={email})', SUBTREE, attributes=['cn', 'title', 'department'])\n",
    "        \n",
    "        logger.info(f\"LDAP lookup for: {identifiers}\")\n",
    "        return []\n",
    "\n",
    "\n",
    "class ZoomInfoService:\n",
    "    \"\"\"\n",
    "    ZoomInfo Service for corporate client persona extraction.\n",
    "    \n",
    "    Looks up external client information from ZoomInfo API.\n",
    "    \n",
    "    Features:\n",
    "    - Email-based lookup: Direct profile match\n",
    "    - Name-based lookup: Fuzzy match with company ranking\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.api_url = os.getenv(\"ZOOMINFO_API_URL\")\n",
    "        self.api_key = os.getenv(\"ZOOMINFO_API_KEY\")\n",
    "    \n",
    "    def get_user_details_from_zoominfo(self, email: str = None, name: str = None):\n",
    "        \"\"\"\n",
    "        Get user details from ZoomInfo API.\n",
    "        \n",
    "        Args:\n",
    "            email: Email address for direct lookup\n",
    "            name: Name for fuzzy lookup\n",
    "            \n",
    "        Returns:\n",
    "            ZoomInfo API response\n",
    "        \"\"\"\n",
    "        # Placeholder - in production this would call ZoomInfo API\n",
    "        # headers = {\"Authorization\": f\"Bearer {self.api_key}\"}\n",
    "        # if email:\n",
    "        #     response = requests.get(f\"{self.api_url}/person/search\", params={\"email\": email}, headers=headers)\n",
    "        # elif name:\n",
    "        #     response = requests.get(f\"{self.api_url}/person/search\", params={\"fullName\": name}, headers=headers)\n",
    "        \n",
    "        logger.info(f\"ZoomInfo lookup for: {email or name}\")\n",
    "        \n",
    "        # Return mock response structure\n",
    "        class MockResponse:\n",
    "            status_code = 200\n",
    "            def json(self):\n",
    "                return {\"data\": []}\n",
    "        return MockResponse()\n",
    "    \n",
    "    def lookup_by_names(self, names_string: str) -> List[Dict[str, Any]]:\n",
    "        \"\"\"\n",
    "        Lookup profiles by name string.\n",
    "        \n",
    "        Args:\n",
    "            names_string: Comma-separated names\n",
    "            \n",
    "        Returns:\n",
    "            List of matching profiles\n",
    "        \"\"\"\n",
    "        names = [n.strip() for n in names_string.split(\",\")]\n",
    "        results = []\n",
    "        for name in names:\n",
    "            response = self.get_user_details_from_zoominfo(name=name)\n",
    "            if response.status_code == 200:\n",
    "                data = response.json().get(\"data\", [])\n",
    "                results.extend(data)\n",
    "        return results\n",
    "\n",
    "\n",
    "def rank_profiles_by_company(profiles: List[Dict], company_name: str) -> List[Dict]:\n",
    "    \"\"\"\n",
    "    Rank ZoomInfo profiles by similarity to target company.\n",
    "    \n",
    "    When searching by name, multiple profiles may be returned.\n",
    "    This function ranks them by company match quality.\n",
    "    \n",
    "    Args:\n",
    "        profiles: List of ZoomInfo profiles\n",
    "        company_name: Target company to match\n",
    "        \n",
    "    Returns:\n",
    "        Profiles sorted by company similarity (highest first)\n",
    "    \"\"\"\n",
    "    if not profiles or not company_name:\n",
    "        return profiles\n",
    "    \n",
    "    for profile in profiles:\n",
    "        company_from_profile = profile.get('company', '') or profile.get('department', '')\n",
    "        if company_from_profile:\n",
    "            similarity = SequenceMatcher(\n",
    "                None, \n",
    "                company_name.lower(), \n",
    "                company_from_profile.lower()\n",
    "            ).ratio()\n",
    "            profile['company_similarity_score'] = similarity\n",
    "        else:\n",
    "            profile['company_similarity_score'] = 0.0\n",
    "    \n",
    "    return sorted(profiles, key=lambda x: x.get('company_similarity_score', 0), reverse=True)\n",
    "\n",
    "\n",
    "# Example FlowForge steps for persona extraction (optional)\n",
    "# These can be added to the chain if persona extraction is needed\n",
    "\n",
    "# @forge.step(\n",
    "#     name=\"extract_rbc_persona\",\n",
    "#     deps=[\"extract_company_info\"],\n",
    "#     produces=[\"rbc_persona\"],\n",
    "#     description=\"Extract RBC employee persona from LDAP\",\n",
    "#     timeout_ms=10000,\n",
    "#     group=\"context_builder\"\n",
    "# )\n",
    "# async def extract_rbc_persona(ctx: ChainContext) -> Dict:\n",
    "#     \"\"\"Extract RBC employee persona using LDAP\"\"\"\n",
    "#     rbc_email = ctx.get(\"rbc_employee_email\")\n",
    "#     if not rbc_email:\n",
    "#         return {\"rbc_persona\": None}\n",
    "#     \n",
    "#     ldap_service = LDAPService(LDAPEmailStrategy())\n",
    "#     profiles = ldap_service.lookup([rbc_email])\n",
    "#     persona = profiles[0] if profiles else None\n",
    "#     ctx.set(\"rbc_persona\", persona, scope=ContextScope.CHAIN)\n",
    "#     return {\"rbc_persona\": persona}\n",
    "\n",
    "\n",
    "# @forge.step(\n",
    "#     name=\"extract_client_persona\",\n",
    "#     deps=[\"extract_company_info\"],\n",
    "#     produces=[\"corporate_client_persona\"],\n",
    "#     description=\"Extract corporate client persona from ZoomInfo\",\n",
    "#     timeout_ms=15000,\n",
    "#     group=\"context_builder\"\n",
    "# )\n",
    "# async def extract_client_persona(ctx: ChainContext) -> Dict:\n",
    "#     \"\"\"Extract corporate client persona using ZoomInfo\"\"\"\n",
    "#     client_email = ctx.get(\"client_email\")\n",
    "#     client_names = ctx.get(\"client_names\")\n",
    "#     company_name = ctx.get(\"company_name\")\n",
    "#     \n",
    "#     zoom_service = ZoomInfoService()\n",
    "#     \n",
    "#     if client_email:\n",
    "#         response = zoom_service.get_user_details_from_zoominfo(email=client_email)\n",
    "#         profiles = response.json().get(\"data\", []) if response.status_code == 200 else []\n",
    "#     elif client_names and company_name:\n",
    "#         profiles = zoom_service.lookup_by_names(client_names)\n",
    "#         profiles = rank_profiles_by_company(profiles, company_name)\n",
    "#     else:\n",
    "#         profiles = []\n",
    "#     \n",
    "#     persona = profiles[0] if profiles else None\n",
    "#     ctx.set(\"corporate_client_persona\", persona, scope=ContextScope.CHAIN)\n",
    "#     return {\"corporate_client_persona\": persona}\n",
    "\n",
    "\n",
    "print(\"\u2705 Persona extraction services defined!\")\n",
    "print(\"   - LookupServiceInterface: Base interface\")\n",
    "print(\"   - LDAPService: RBC employee lookup\")\n",
    "print(\"   - ZoomInfoService: External client lookup\")\n",
    "print(\"   - rank_profiles_by_company(): Company matching helper\")\n",
    "print(\"\\n\ud83d\udccc Note: Persona extraction steps are commented out.\")\n",
    "print(\"   Uncomment and add to chain.steps to enable.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"22-testing\"></a>\n",
    "## 22. Testing Strategies\n",
    "\n",
    "FlowForge supports several testing approaches for chain validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\n",
    "#                           TESTING STRATEGIES\n",
    "# \u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\n",
    "\n",
    "\"\"\"\n",
    "FlowForge Testing Patterns:\n",
    "\n",
    "1. Unit Testing: Test individual steps in isolation\n",
    "2. Integration Testing: Test full chain with mocked agents\n",
    "3. Validation Testing: Use forge.check() for structural validation\n",
    "4. End-to-End Testing: Full chain with real/mock external services\n",
    "\"\"\"\n",
    "\n",
    "# Example: Unit test for a step\n",
    "async def test_extract_company_info():\n",
    "    \"\"\"Unit test example for extract_company_info step\"\"\"\n",
    "    from flowforge.core.context import ChainContext, ContextManager\n",
    "    \n",
    "    # Create isolated context\n",
    "    ctx = ContextManager().create_context(\"test_chain\")\n",
    "    ctx.set(\"company_name\", \"Test Corp\")\n",
    "    \n",
    "    # Run the step function directly\n",
    "    result = await extract_company_info(ctx)\n",
    "    \n",
    "    # Assertions\n",
    "    assert \"company_info\" in result\n",
    "    assert result[\"company_info\"][\"company_name\"] == \"Test Corp\"\n",
    "    print(\"\u2705 Unit test passed!\")\n",
    "\n",
    "\n",
    "# Example: Integration test with mocked services\n",
    "async def test_cmpt_integration():\n",
    "    \"\"\"Integration test using FlowForge's isolated mode\"\"\"\n",
    "    \n",
    "    # Create isolated forge instance (prevents state bleed)\n",
    "    test_forge = FlowForge(name=\"cmpt_test\", isolated=True)\n",
    "    \n",
    "    # Register steps (same as main implementation)\n",
    "    # ... (steps would be registered here)\n",
    "    \n",
    "    # Run chain with test data\n",
    "    result = await test_forge.launch(\n",
    "        \"cmpt_pipeline\",\n",
    "        data={\n",
    "            \"company_name\": \"Apple Inc\",\n",
    "            \"meeting_date\": \"2025-02-15\"\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    # Assertions\n",
    "    assert result.get(\"success\") is True\n",
    "    print(\"\u2705 Integration test passed!\")\n",
    "\n",
    "\n",
    "# Validation test\n",
    "def test_chain_validation():\n",
    "    \"\"\"Test chain structure using forge.check()\"\"\"\n",
    "    validation = forge.check(\"cmpt_pipeline\")\n",
    "    \n",
    "    assert validation[\"valid\"] is True, f\"Validation failed: {validation.get('errors')}\"\n",
    "    assert len(validation.get(\"errors\", [])) == 0\n",
    "    print(\"\u2705 Validation test passed!\")\n",
    "\n",
    "\n",
    "# Run validation test\n",
    "test_chain_validation()\n",
    "\n",
    "print(\"\\n\ud83d\udccb Testing strategies available:\")\n",
    "print(\"   - test_extract_company_info(): Unit test example\")\n",
    "print(\"   - test_cmpt_integration(): Integration test example\")\n",
    "print(\"   - test_chain_validation(): Structural validation test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## \ud83d\udcda Complete Summary: Old Code \u2192 FlowForge Migration\n",
    "\n",
    "### What Was Migrated from `requirements_and_old_code.MD`:\n",
    "\n",
    "| Old Code Component | FlowForge Implementation | Section |\n",
    "|--------------------|--------------------------|---------|\n",
    "| `ChainServerRequest` | `CMPTRequest` model | \u00a75 |\n",
    "| `FinancialMetricsResponse`, `StrategicAnalysisResponse` | Pydantic models with CitationDict | \u00a75 |\n",
    "| `grid_config.py` (GRID) | `GRID_CONFIG` dictionary | \u00a76 |\n",
    "| `ContextBuilderService.corporate_client_firm_extractor` | `extract_company_info` step | \u00a711 |\n",
    "| `ContextBuilderService.temporal_content_extractor` | `extract_temporal_context` step | \u00a711 |\n",
    "| `ContentPrioritizationService.temporal_source_prioritizer` | `temporal_source_prioritizer` step | \u00a712 |\n",
    "| `StaticSubqueryEngine.get_subquery_arguments` | `generate_subqueries` step + `StaticSubqueryEngine` class | \u00a712, \u00a721.2 |\n",
    "| `ResponseBuilderAndGenerator.context_parser` | `parse_agent_data` step | \u00a714 |\n",
    "| `ResponseBuilderAndGenerator.prompt_builder` | `build_prompts` step | \u00a714 |\n",
    "| `ResponseBuilderAndGenerator.get_structured_response` | `generate_financial_metrics`, `generate_strategic_analysis` steps | \u00a714 |\n",
    "| `MetricsValidator.validate_financial_metrics` | `validate_metrics` step + `MetricsValidator` class | \u00a714, \u00a721.1 |\n",
    "| `ChainOrchestrator.execute_chain` | `CMPTPipeline` chain + `build_response` step | \u00a710 |\n",
    "| `llm_prompts.py` | LLM prompt templates and builders | \u00a721.3 |\n",
    "| `ldap_service.py`, `zoom_info_service.py` | `LDAPService`, `ZoomInfoService` classes | \u00a721.4 |\n",
    "\n",
    "### FlowForge Benefits Over Old Code:\n",
    "\n",
    "| Feature | Old Code | FlowForge |\n",
    "|---------|----------|-----------|\n",
    "| **Execution** | Sequential service calls | Automatic DAG parallelization |\n",
    "| **Dependencies** | Manual orchestration | Declarative `deps=[]` |\n",
    "| **Context** | Passed between functions | `ChainContext` with scoped storage |\n",
    "| **Error Handling** | Try/catch in each service | Configurable `fail_fast` vs `continue` |\n",
    "| **Validation** | Manual | Built-in `forge.check()` |\n",
    "| **Visualization** | None | ASCII/Mermaid DAG diagrams |\n",
    "| **Testing** | Per-service tests | Isolated forge instances |\n",
    "\n",
    "### Pipeline Execution Comparison:\n",
    "\n",
    "**Old Code (Sequential):**\n",
    "```\n",
    "ContextBuilder.execute() \u2192 ContentPrioritization.execute() \u2192 ResponseBuilder.execute()\n",
    "```\n",
    "\n",
    "**FlowForge (Parallel where possible):**\n",
    "```\n",
    "extract_company_info \u2192 extract_temporal_context \u2192 build_context\n",
    "                                                       \u2193\n",
    "              \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n",
    "              \u2193                                                              \u2193\n",
    "temporal_source_prioritizer                                    generate_subqueries\n",
    "              \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n",
    "                                         \u2193\n",
    "                                  prioritize_content\n",
    "                                         \u2193\n",
    "           \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n",
    "           \u2193                       \u2193           \u2193                       \u2193\n",
    "    fetch_news_data         fetch_sec_data    fetch_earnings_data\n",
    "           \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n",
    "                                         \u2193\n",
    "                                  parse_agent_data \u2192 build_prompts\n",
    "                                                          \u2193\n",
    "                           \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n",
    "                           \u2193                                                             \u2193\n",
    "               generate_financial_metrics                              generate_strategic_analysis\n",
    "                           \u2193                                                             \u2502\n",
    "                    validate_metrics \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n",
    "                                                                                         \u2193\n",
    "                                                                                  build_response\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## \ud83d\ude80 Production Deployment Checklist\n",
    "\n",
    "### Environment Variables Required:\n",
    "```bash\n",
    "# Foundation Services\n",
    "FOUNDATION_COMPANY_MATCHES=https://...\n",
    "FOUNDATION_EARNING_CALENDAR_URL=https://...\n",
    "\n",
    "# MCP Agents\n",
    "NEWS_AGENT_MCP_URL=https://...\n",
    "NEWS_AGENT_MCP_BEARER_TOKEN=...\n",
    "SEC_AGENT_MCP_URL=https://...\n",
    "SEC_AGENT_MCP_BEARER_TOKEN=...\n",
    "EARNINGS_AGENT_MCP_URL=https://...\n",
    "EARNINGS_AGENT_MCP_BEARER_TOKEN=...\n",
    "\n",
    "# LLM Gateway\n",
    "LLM_SERVER_URL=https://...\n",
    "LLM_OAUTH_ENDPOINT=https://...\n",
    "LLM_CLIENT_ID=...\n",
    "LLM_CLIENT_SECRET=...\n",
    "\n",
    "# Optional: Persona Services\n",
    "LDAP_SERVER=...\n",
    "ZOOMINFO_API_URL=...\n",
    "ZOOMINFO_API_KEY=...\n",
    "```\n",
    "\n",
    "### Production Enhancements:\n",
    "1. **Replace placeholder `_execute_query` methods** with real MCP client calls\n",
    "2. **Connect LLM Gateway** for metrics/analysis generation\n",
    "3. **Enable caching middleware** for repeated queries\n",
    "4. **Add monitoring/metrics** middleware\n",
    "5. **Configure retry/timeout** per step based on SLAs\n",
    "6. **Enable persona extraction** if needed\n",
    "\n",
    "---\n",
    "\n",
    "## \ud83d\udcd6 Reference: FlowForge API Quick Guide\n",
    "\n",
    "| API | Description | Example |\n",
    "|-----|-------------|---------|\n",
    "| `FlowForge(name, ...)` | Create forge instance | `forge = FlowForge(name=\"cmpt\")` |\n",
    "| `@forge.agent(name)` | Register data agent class | `@forge.agent(name=\"news_agent\")` |\n",
    "| `@forge.step(name, deps, produces)` | Register chain step | `@forge.step(name=\"fetch\", deps=[\"build\"])` |\n",
    "| `@forge.chain(name)` | Register chain definition | `@forge.chain(name=\"pipeline\")` |\n",
    "| `forge.check(chain_name)` | Validate chain | `forge.check(\"pipeline\")` |\n",
    "| `forge.graph(chain_name)` | Visualize DAG | `forge.graph(\"pipeline\", format=\"ascii\")` |\n",
    "| `forge.launch(chain_name, data)` | Execute chain | `await forge.launch(\"pipeline\", data={...})` |\n",
    "| `forge.get_agent(name)` | Get registered agent | `agent = forge.get_agent(\"news_agent\")` |\n",
    "| `forge.list_agents()` | List all agents | `forge.list_agents()` |\n",
    "| `ctx.get(key)` | Read from context | `value = ctx.get(\"company_info\")` |\n",
    "| `ctx.set(key, value, scope)` | Write to context | `ctx.set(\"data\", val, scope=ContextScope.CHAIN)` |\n",
    "\n",
    "---\n",
    "\n",
    "**End of Tutorial**\n",
    "\n",
    "This notebook provides a complete implementation of the CMPT chain using FlowForge, migrating all components from the original `requirements_and_old_code.MD` file."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}